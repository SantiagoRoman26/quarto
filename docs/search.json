[
  {
    "objectID": "midterm.html",
    "href": "midterm.html",
    "title": "MidTerm",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n\n\n\n\n\n\nCode\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\nprint(\"Shape X:\", X.shape, \"Shape y:\", y.shape)\n\n\nShape X: (20640, 8) Shape y: (20640,)\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\n\n\n\nCode\nreg_pipe = Pipeline([\n    (\"scaler\", StandardScaler()), \n    (\"regressor\", LinearRegression())\n])\n\n\n\n\n\n\n\nCode\nreg_pipe.fit(X_train, y_train)\n\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('regressor', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('scaler', ...), ('regressor', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_prep_pipe = reg_pipe.predict(X_test)\n\n\n\n\n\n\n\nCode\ntrain_sizes, train_scores, test_scores = learning_curve(\n    reg_pipe, X, y, cv=5, scoring=\"r2\", n_jobs=1\n)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\n\nplt.plot(train_sizes, train_mean, label=\"Entrenamiento\")\nplt.plot(train_sizes, test_mean, label=\"Validación\")\nplt.xlabel(\"Tamaño del conjunto de entrenamiento\")\nplt.ylabel(\"R² Score\")\nplt.title(\"Curva de Aprendizaje\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nLa curva de aprendizaje se realizo tomando en cuenta el valor R2 el cual representa el porcentaje del comportamiento de los datos que el modelo logra entender. mostrando que el modelo aprende los patrones para poder hacer predicciones.\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_prep_pipe, color='blue', alpha=0.7, s=10)\nplt.title('Cargos reales vs. Cargos predichos')\nplt.xlabel('Valores reales')\nplt.ylabel('Valores predichos')\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Línea de referencia\nplt.legend(['Valores predichos', 'Línea de referencia'])\nplt.show()\n\n\n\n\n\n\n\n\n\nEsta grafica muestra la distribución de los datos al hacer la regresión lineal, donde se muestra los valores predichos y valores reales, además de una linea de referencia que muestra cuando los datos son predichos correctamente. En este caso los puntos se agrupan cerca de la linea de referencia aunque con cierta dispersión, lo cual muestra que el modelo acierta de forma general en gran parte de los datos, pero tiene errores en valores extremos.\n\n\nCode\nmae = mean_absolute_error(y_test, y_prep_pipe)\nprint(f\"Error medio absoluto (MAE): {mae:.3f}\")\n\n\nError medio absoluto (MAE): 0.533\n\n\n\n\n\nSe usa un nuevo registro para mostrar como funciona la prediccion.\n\n\nCode\nnuevo_registro = pd.DataFrame([[\n    8.3252,   # MedInc - ingreso medio\n    41.0,     # HouseAge - edad promedio de las casas\n    6.9841,   # AveRooms - promedio de habitaciones\n    1.0238,   # AveBedrms - promedio de dormitorios\n    322.0,    # Population - población\n    2.5556,   # AveOccup - ocupación promedio\n    37.88,    # Latitude\n    -122.23   # Longitude\n]], columns=housing.feature_names)\n\nprediccion = reg_pipe.predict(nuevo_registro)\nprint(f'Predicción del valor medio de la vivienda: ${prediccion[0]* 100000:.2f}')\n\n\nPredicción del valor medio de la vivienda: $415193.84\n\n\nc:\\respaldo para reparacion de disco\\RESPALDO\\StarMedia\\desarrollo\\Maestria Inteligencia artificial\\Clase Inicial\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, RocCurveDisplay,ConfusionMatrixDisplay\n\nimport seaborn as sns\nimport pandas as pd\n\n\n\n\n\n\n\nCode\nurl = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n\n# Según el dataset, las columnas serían:\ncol_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \n            \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\ndf = pd.read_csv(url, header=None, names=col_names)\n\ncols_with_zeros = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\ndf[cols_with_zeros] = df[cols_with_zeros].replace(0, np.nan)\ndf = df.dropna()\n\nX = df.drop(\"Outcome\", axis=1).values\ny = df[\"Outcome\"].values\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\n\n\n\nCode\nclf_pipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"classifier\", LogisticRegression(max_iter=500))\n])\n\n\n\n\n\n\n\nCode\nclf_pipe.fit(X_train, y_train)\n\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('classifier', LogisticRegression(max_iter=500))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('scaler', ...), ('classifier', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n500\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_prep_pipe = clf_pipe.predict(X_test)\n\n\n\n\n\n\n\nCode\nprint(\"Accuracy:\", accuracy_score(y_test, y_prep_pipe))\nprint(\"Precision:\", precision_score(y_test, y_prep_pipe))\nprint(\"Recall:\", recall_score(y_test, y_prep_pipe))\nprint(\"F1 Score:\", f1_score(y_test, y_prep_pipe))\n\n\nAccuracy: 0.7721518987341772\nPrecision: 0.6956521739130435\nRecall: 0.5925925925925926\nF1 Score: 0.64\n\n\n\n\n\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_prep_pipe)\n\n\n\n\n\n\n\n\n\nEsta matriz de confusion muestra la cantidad de datos que se llegan a predecir, asi se muestra que en realidad el modelo muestra lo siguiente:\n\n45 casos fueron correctamente clasificados como No Diabetes\n16 casos fueron correctamente identificados como Diabetes\n11 casos presentaron Falsos Negativos\n8 casos presentaron Falsos Positivos\n\n\n\n\n\n\nCode\nRocCurveDisplay.from_estimator(clf_pipe, X_test, y_test)\nplt.show()\n\n\n\n\n\n\n\n\n\nLa Curva ROC evalúa el rendimiento del modelo de clasificación mostrando la relación entre la Tasa de Verdaderos Positivos y la Tasa de Falsos Positivos en diferentes umbrales de decisión. El modelo obtiene un AUC de 0.83, lo que indica un 83% de probabilidad de clasificar correctamente un caso positivo por encima de uno negativo, siendo considerado un rendimiento bueno.",
    "crumbs": [
      "MidTerm"
    ]
  },
  {
    "objectID": "midterm.html#problema-de-regresión",
    "href": "midterm.html#problema-de-regresión",
    "title": "MidTerm",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n\n\n\n\n\n\nCode\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\nprint(\"Shape X:\", X.shape, \"Shape y:\", y.shape)\n\n\nShape X: (20640, 8) Shape y: (20640,)\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\n\n\n\nCode\nreg_pipe = Pipeline([\n    (\"scaler\", StandardScaler()), \n    (\"regressor\", LinearRegression())\n])\n\n\n\n\n\n\n\nCode\nreg_pipe.fit(X_train, y_train)\n\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('regressor', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('scaler', ...), ('regressor', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_prep_pipe = reg_pipe.predict(X_test)\n\n\n\n\n\n\n\nCode\ntrain_sizes, train_scores, test_scores = learning_curve(\n    reg_pipe, X, y, cv=5, scoring=\"r2\", n_jobs=1\n)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\n\nplt.plot(train_sizes, train_mean, label=\"Entrenamiento\")\nplt.plot(train_sizes, test_mean, label=\"Validación\")\nplt.xlabel(\"Tamaño del conjunto de entrenamiento\")\nplt.ylabel(\"R² Score\")\nplt.title(\"Curva de Aprendizaje\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nLa curva de aprendizaje se realizo tomando en cuenta el valor R2 el cual representa el porcentaje del comportamiento de los datos que el modelo logra entender. mostrando que el modelo aprende los patrones para poder hacer predicciones.\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_prep_pipe, color='blue', alpha=0.7, s=10)\nplt.title('Cargos reales vs. Cargos predichos')\nplt.xlabel('Valores reales')\nplt.ylabel('Valores predichos')\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Línea de referencia\nplt.legend(['Valores predichos', 'Línea de referencia'])\nplt.show()\n\n\n\n\n\n\n\n\n\nEsta grafica muestra la distribución de los datos al hacer la regresión lineal, donde se muestra los valores predichos y valores reales, además de una linea de referencia que muestra cuando los datos son predichos correctamente. En este caso los puntos se agrupan cerca de la linea de referencia aunque con cierta dispersión, lo cual muestra que el modelo acierta de forma general en gran parte de los datos, pero tiene errores en valores extremos.\n\n\nCode\nmae = mean_absolute_error(y_test, y_prep_pipe)\nprint(f\"Error medio absoluto (MAE): {mae:.3f}\")\n\n\nError medio absoluto (MAE): 0.533\n\n\n\n\n\nSe usa un nuevo registro para mostrar como funciona la prediccion.\n\n\nCode\nnuevo_registro = pd.DataFrame([[\n    8.3252,   # MedInc - ingreso medio\n    41.0,     # HouseAge - edad promedio de las casas\n    6.9841,   # AveRooms - promedio de habitaciones\n    1.0238,   # AveBedrms - promedio de dormitorios\n    322.0,    # Population - población\n    2.5556,   # AveOccup - ocupación promedio\n    37.88,    # Latitude\n    -122.23   # Longitude\n]], columns=housing.feature_names)\n\nprediccion = reg_pipe.predict(nuevo_registro)\nprint(f'Predicción del valor medio de la vivienda: ${prediccion[0]* 100000:.2f}')\n\n\nPredicción del valor medio de la vivienda: $415193.84\n\n\nc:\\respaldo para reparacion de disco\\RESPALDO\\StarMedia\\desarrollo\\Maestria Inteligencia artificial\\Clase Inicial\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n  warnings.warn(",
    "crumbs": [
      "MidTerm"
    ]
  },
  {
    "objectID": "midterm.html#problema-de-clasificación",
    "href": "midterm.html#problema-de-clasificación",
    "title": "MidTerm",
    "section": "",
    "text": "Code\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, RocCurveDisplay,ConfusionMatrixDisplay\n\nimport seaborn as sns\nimport pandas as pd\n\n\n\n\n\n\n\nCode\nurl = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n\n# Según el dataset, las columnas serían:\ncol_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \n            \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\ndf = pd.read_csv(url, header=None, names=col_names)\n\ncols_with_zeros = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\ndf[cols_with_zeros] = df[cols_with_zeros].replace(0, np.nan)\ndf = df.dropna()\n\nX = df.drop(\"Outcome\", axis=1).values\ny = df[\"Outcome\"].values\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\n\n\n\nCode\nclf_pipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"classifier\", LogisticRegression(max_iter=500))\n])\n\n\n\n\n\n\n\nCode\nclf_pipe.fit(X_train, y_train)\n\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('classifier', LogisticRegression(max_iter=500))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('scaler', ...), ('classifier', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n500\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_prep_pipe = clf_pipe.predict(X_test)\n\n\n\n\n\n\n\nCode\nprint(\"Accuracy:\", accuracy_score(y_test, y_prep_pipe))\nprint(\"Precision:\", precision_score(y_test, y_prep_pipe))\nprint(\"Recall:\", recall_score(y_test, y_prep_pipe))\nprint(\"F1 Score:\", f1_score(y_test, y_prep_pipe))\n\n\nAccuracy: 0.7721518987341772\nPrecision: 0.6956521739130435\nRecall: 0.5925925925925926\nF1 Score: 0.64\n\n\n\n\n\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_prep_pipe)\n\n\n\n\n\n\n\n\n\nEsta matriz de confusion muestra la cantidad de datos que se llegan a predecir, asi se muestra que en realidad el modelo muestra lo siguiente:\n\n45 casos fueron correctamente clasificados como No Diabetes\n16 casos fueron correctamente identificados como Diabetes\n11 casos presentaron Falsos Negativos\n8 casos presentaron Falsos Positivos\n\n\n\n\n\n\nCode\nRocCurveDisplay.from_estimator(clf_pipe, X_test, y_test)\nplt.show()\n\n\n\n\n\n\n\n\n\nLa Curva ROC evalúa el rendimiento del modelo de clasificación mostrando la relación entre la Tasa de Verdaderos Positivos y la Tasa de Falsos Positivos en diferentes umbrales de decisión. El modelo obtiene un AUC de 0.83, lo que indica un 83% de probabilidad de clasificar correctamente un caso positivo por encima de uno negativo, siendo considerado un rendimiento bueno.",
    "crumbs": [
      "MidTerm"
    ]
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "Laboratorio 1",
    "section": "",
    "text": "Actividad: Lab 1\n\nImportar el DataSet a usar\n\n\nCode\nimport altair as alt\nfrom vega_datasets import data\nimport pandas as pd\n\n# Carga con el separador correcto y encoding\ntry:\n    datos = pd.read_csv(\"rnd_01102021.csv\", encoding='latin-1', sep=';')\nexcept:\n    try:\n        datos = pd.read_csv(\"rnd_01102021.csv\", encoding='utf-8', sep=';')\n    except:\n        datos = pd.read_csv(\"rnd_01102021.csv\", sep=';')\n\n# Verifica la estructura\nprint(datos.head())\n\n\n   Recuento Grado de Discapacidad     Género  Provincia  \\\n0         1          De 75% a 84%  Masculino   Imbabura   \n1         1          De 75% a 84%  Masculino  Pichincha   \n2         1          De 50% a 74%   Femenino  Pichincha   \n3         1          De 50% a 74%   Femenino  Pichincha   \n4         1          De 75% a 84%  Masculino     Carchi   \n\n             Grupo Etareo Tipo de Discapacidad  \n0         De 36 a 64 años               Física  \n1         De 36 a 64 años               Física  \n2  De 65 años en adelante               Física  \n3  De 65 años en adelante               Física  \n4  De 65 años en adelante               Física  \n\n\n\n\nDistribucion de personas con capacidad por provincia y genero\n\n\nCode\nchart1 = alt.Chart(datos).mark_bar().encode(\n    x=alt.X('Provincia:N', sort='-y', title='Provincia'),\n    y=alt.Y('sum(Recuento):Q', title='Número de Personas'),\n    color='Género:N',\n    tooltip=['Provincia', 'Género', 'sum(Recuento)']\n).properties(\n    title='Distribución por Provincia y Género',\n    width=600, \n    height=400\n)\n\nchart1\n\n\n\n\n\n\n\n\n\n\nHeatmap de relación entre tipo de Discapacidad y grupo de Edad\n\n\n\nCode\nheatmap_data = datos.groupby(['Tipo de Discapacidad', 'Grupo Etareo'])['Recuento'].sum().reset_index()\n\nchart2 = alt.Chart(heatmap_data).mark_rect().encode(\n    x='Tipo de Discapacidad:N',\n    y='Grupo Etareo:N',\n    color=alt.Color('Recuento:Q', scale=alt.Scale(scheme='blues')),\n    tooltip=['Tipo de Discapacidad', 'Grupo Etareo', 'Recuento']\n).properties(\n    title='Relación entre Tipo de Discapacidad y Grupo de Edad',\n    width=600, \n    height=400\n)\n\nchart2\n\n\n\n\n\n\n\n\n\n\nDistribución por tipo de Discapacidad\n\n\nCode\nchart3 = alt.Chart(datos).mark_arc().encode(\n    theta='sum(Recuento):Q',\n    color='Tipo de Discapacidad:N',\n    tooltip=['Tipo de Discapacidad', 'sum(Recuento)']\n).properties(\n    title='Distribución por Tipo de Discapacidad'\n)\n\nchart3",
    "crumbs": [
      "Laboratorios",
      "Laboratorio 1"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is one of the fundamental steps in any data science process. It allows us to understand the structure, detect anomalies, and uncover patterns in the data before modeling.\n\n“Without EDA, you’re not doing data science, you’re just guessing.”\n\nEDA combines statistics, programming, and visualization to explore datasets. This report is designed to help you practice these core skills using real-world data.\n\n\nWe will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet’s load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let’s examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset.\n\n\n\nBefore diving deeper into the data, it’s useful to explore some key metadata:\n\n✅ The column names and their data types\n⚠️ The presence of missing values\n📊 Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we’re dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.2+ KB\n\n\n\n\n\n\nDetecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet’s start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows × 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let’s set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis.\n\n\n\n\nFinally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows × 15 columns",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "eda.html#dataset",
    "href": "eda.html#dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "We will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet’s load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let’s examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset.",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "eda.html#first-steps",
    "href": "eda.html#first-steps",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Before diving deeper into the data, it’s useful to explore some key metadata:\n\n✅ The column names and their data types\n⚠️ The presence of missing values\n📊 Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we’re dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.2+ KB",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "eda.html#missing-values",
    "href": "eda.html#missing-values",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Detecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet’s start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows × 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let’s set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis.",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "eda.html#cleaned-dataset",
    "href": "eda.html#cleaned-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Finally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows × 15 columns",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "eda.html#univariate-analysis-quantitative",
    "href": "eda.html#univariate-analysis-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.1 Univariate Analysis: Quantitative",
    "text": "2.1 Univariate Analysis: Quantitative\nA univariate analysis focuses on examining a single numeric variable to understand its distribution, shape, central tendency, and spread. One of the most common tools for this is the histogram.\nIn this case, we’ll explore the distribution of the movie runtime (Running_Time_min).\n\n2.1.1 Basic Histogram\nWe start by creating a histogram to visualize the distribution of running times:\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    title='Histogram of Movie Runtimes (30 bins)'\n)\n\n\n\n\n\n\n\n\nThis chart shows how many movies fall into each time interval (bin). However, histograms can look quite different depending on the number and size of bins used.\n\n\n2.1.2 Effect of Bin Size\nLet’s compare how the histogram shape changes with different bin sizes:\n\n\nCode\nhistogram_1 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=8)),\n    alt.Y('count()')\n)\n\nhistogram_2 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=10)),\n    alt.Y('count()')\n)\n\nhistogram_1 | histogram_2\n\n\n\n\n\n\n\n\nEven though both plots use the same data, the choice of bin size changes the visual interpretation. A small number of bins may hide details, while too many bins can make it harder to spot trends.\n\n\n2.1.3 Density plots, or Kernel Density Estimate (KDE)\nDensity plots offer a smoothed alternative to histograms. Instead of using rectangular bins to count data points, they estimate the probability density function by placing bell-shaped curves (kernels) at each observation and summing them.\nThis approach helps reduce the visual noise and jaggedness that can occur in histograms and gives a clearer picture of the underlying distribution.\n\n\nCode\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    as_=['Running_Time_min','density'],\n).mark_area().encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q')\n).properties(\n    title=\"Movies runtime\"\n)\n\n\n\n\n\n\n\n\n\n\n2.1.4 Grouped Density plot\nWe can also compare distributions across groups by splitting the KDE by a categorical variable using the groupby parameter. This helps us see how the distribution differs between categories, such as genres.\n\n\nCode\nselection = alt.selection_point(fields=['Major_Genre'], bind='legend')\n\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    groupby=['Major_Genre'],\n    as_=['Running_Time_min', 'density'],\n).mark_area(opacity=0.5).encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q', stack=None),\n    alt.Color('Major_Genre'),\n    opacity=alt.condition(selection, \n        alt.value(1), \n        alt.value(0.05)\n    )\n).add_params(\n    selection\n).properties(\n    title=\"Movies Runtime by Genre (Interactive Filter)\"\n).interactive()\n\n\n\n\n\n\n\n\nThe transparency (opacity=0.5) allows us to observe overlapping distributions and ensures that small density areas are not completely hidden behind larger ones.\nFrom this plot, we can observe, for example, that Drama movies have runtimes nearly as long as the longest Adventure movies, even though their overall distributions differ.",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "href": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.2 Bivariate Analysis: Categorical vs Quantitative",
    "text": "2.2 Bivariate Analysis: Categorical vs Quantitative\nBivariate analysis examines the relationship between two variables. In this case, we focus on one categorical variable (e.g., genre) and one quantitative variable (e.g., revenue), which is a very common scenario in exploratory data analysis.\nThis type of analysis is useful to: - Compare average or median values across categories. - Detect outliers or high-variance groups. - Understand distributional differences across categories.\nBelow are several effective visualizations for this analysis.\n\n2.2.1 Basic Barchart\nBar charts are effective for comparing aggregated values (like the mean) across different groups. However, they hide the distribution and variation within each group.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Average Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\nThis bar chart shows the mean Worldwide Gross per genre. It is useful for identifying which genres are more profitable on average, but does not show how spread out the data is.\n\n\n2.2.2 Tick Plot\nTo visualize individual data points, we use a tick plot. This helps uncover variability within genres and detect outliers.\n\n\nCode\nalt.Chart(movies_cleaned).mark_tick().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\"),\n    alt.Tooltip('Title:N')\n).properties(\n    title=\"Individual Gross per Movie by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.3 Heatmaps\nHeatmaps can summarize the frequency of data points across both axes (quantitative and categorical) using color intensity. It’s particularly useful for spotting patterns without getting overwhelmed by individual points.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Worldwide_Gross',bin=alt.Bin(maxbins=100)),\n    alt.Y(\"Major_Genre\"),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Heatmap of Movie Counts by Gross and Genre\"\n)\n\n\n\n\n\n\n\n\nThis heatmap shows how frequently movies from each genre fall into different revenue ranges.\n\n\n2.2.4 Boxplot\nBoxplots are useful for comparing distributions across categories and identifying outliers. Boxplots summarize a distribution using five statistics:\n\nMedian (Q2)\nFirst Quartile (Q1)\nThird Quartile (Q3)\nLower Whisker (Q1 - 1.5 × IQR)\nUpper Whisker (Q3 + 1.5 × IQR)\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Boxplot of Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.5 Side-by-side: Boxplot and Bar Chart\nTo contrast aggregated values (bar chart) with the full distribution (boxplot), we can display them together:\n\n\nCode\nbar = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox = alt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox | bar\n\n\n\n\n\n\n\n\nThis comparison reveals whether the mean is a good representative of the genre, or whether the data is skewed or contains outliers that affect the average",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "href": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.3 Bivariate Analysis: Quantitative vs Quantitative",
    "text": "2.3 Bivariate Analysis: Quantitative vs Quantitative\nWhen analyzing two quantitative (numerical) variables simultaneously, we aim to discover possible relationships, trends, or correlations. This type of bivariate analysis can reveal whether increases in one variable are associated with increases or decreases in another (positive or negative correlation), or if there’s no relationship at all. The most common and intuitive visualization for this is the scatterplot.\n\n2.3.1 Scatterplots\nScatter plots are effective visualizations for exploring two-dimensional distributions, allowing us to identify patterns, trends, clusters, or outliers.\nLet’s start by visualizing how movies are rated across two popular online platforms:\n\nIMDb\n\nRotten Tomatoes\n\nAre movies rated similarly on different platforms?\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('IMDB_Rating'),\n    alt.Y('Rotten_Tomatoes_Rating')\n).properties(\n    title=\"IMDB vs Rotten Tomatoes Ratings\"\n)\n\n\n\n\n\n\n\n\n\n\n2.3.2 Scatterplot Saturation\nScatterplots can become saturated when too many points overlap in a small area of the chart, making it difficult to distinguish dense regions from sparse ones. For example, when plotting financial variables like production budget versus worldwide gross:\n\n\nCode\nsaturated = alt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('Production_Budget'),\n    alt.Y('Worldwide_Gross')\n).properties(\n    title=\"Saturated Scatterplot: Budget vs Gross\"\n)\nsaturated\n\n\n\n\n\n\n\n\n\n\n2.3.3 Using Binned Heatmap to Reduce Saturation\nTo address saturation, we can bin both variables and use a heatmap where the color intensity represents the number of movies that fall into each rectangular region of the grid. This makes dense areas more interpretable\n\n\nCode\nheatmap_scatter = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Production_Budget', bin=alt.Bin(maxbins=60)),\n    alt.Y('Worldwide_Gross', bin=alt.Bin(maxbins=60)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Binned Heatmap: Budget vs Gross\"\n)\nheatmap_scatter\n\n\n\n\n\n\n\n\n\n\n2.3.4 Side-by-side Comparison\nCompare the raw scatterplot with the heatmap representation:\n\n\nCode\nsaturated | heatmap_scatter",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "href": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.4 Bivariate Analysis: Categorical vs Categorical",
    "text": "2.4 Bivariate Analysis: Categorical vs Categorical\nWhen working with two categorical variables, bivariate analysis helps us understand how categories from one variable relate or are distributed across the other. For example, we might want to know how different movie genres are rated according to the MPAA rating system. Visualization techniques like grouped bar charts and faceted plots can reveal patterns, associations, or class imbalances.\n\n2.4.1 Basic Faceted Bar Chart\nWe begin by exploring how movies are rated (MPAA_Rating) across different genres (Major_Genre). A faceted bar chart allows us to visualize this relationship by plotting a bar chart per genre, helping to identify genre-specific rating distributions.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre'\n)\n\n\n\n\n\n\n\n\n\n\n2.4.2 Vertical Faceting for Alignment\nFaceting horizontally can make comparisons across genres harder when the x-axis is misaligned. By specifying columns=1, we lay out the facets vertically, making it easier to compare counts across genres.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=1\n)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Dependent vs Independent Axis Scaling\nBy default, facet plots share the same x-axis scale (dependent scale), which allows for easier comparison across panels. However, when the number of observations varies greatly between genres, this shared scale can compress some charts.\nWe can instead use independent x-axis scaling for each facet. This highlights the relative distribution within each genre.\n\n\nCode\nshared_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n)\n\nindependent_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n).resolve_scale(x='independent')\n\nshared_scale | independent_scale\n\n\n\n\n\n\n\n\nThe left panel (shared scale) makes absolute comparisons between genres, while the right panel (independent scale) makes within-genre comparisons more readable.\n\n\n2.4.4 Heatmaps\nHeatmaps are effective for visualizing the relationship between two categorical variables when the goal is to display counts or frequency of occurrences. They map the number of observations to color, providing an intuitive view of which category pairs are most or least common.\nWe can enhance this basic representation by also using marker size, combining both color intensity and circle area to represent counts more effectively. This dual encoding can improve interpretation, especially when printed in grayscale or when there are subtle color differences.\n\n\nCode\nheatmap_color = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()')\n).properties(\n    title=\"Heatmap with Color (Count of Movies)\"\n)\n\nheatmap_size = alt.Chart(movies_cleaned).mark_circle().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()'),\n    alt.Size('count()')\n).properties(\n    title=\"Heatmap with Color + Size (Count of Movies)\"\n)\n\nheatmap_color | heatmap_size",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "eda.html#multivariate-analysis",
    "href": "eda.html#multivariate-analysis",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.5 Multivariate Analysis",
    "text": "2.5 Multivariate Analysis\nMultivariate analysis helps us understand the interactions and relationships among multiple variables simultaneously. In the context of numerical features, it is useful to explore pairwise distributions, correlations, and detect potential clusters or anomalies.\nWhen the number of variables is large, repeated charts such as histograms or scatter plot matrices help us summarize patterns efficiently and consistently across all numerical dimensions.\n\n2.5.1 Repeated Histograms for Numerical Columns\nWe first identify and isolate all numerical columns from the dataset. Then we repeat a histogram for each of these columns to understand the individual distributions. This overview is helpful to detect skewness, outliers, or binning decisions that affect how data is grouped visually.\n\n\nCode\n# Select only numerical columns\nnumerical_columns = movies_cleaned.select_dtypes('number').columns.tolist()\n\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X(alt.repeat(),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    width=150,\n    height=150\n).repeat(\n    numerical_columns,\n    columns=4\n)\n\n\n\n\n\n\n\n\n\n\n2.5.2 Scatter Plot Matrix (Pairplot)\nA scatter plot matrix shows the pairwise relationships between all numerical variables. This is a common exploratory tool to detect:\n\nCorrelations between variables\nOutliers or clusters\nRelationships useful for prediction models (e.g., to predict rating or budget)\n\nWe focus especially on the plots below the diagonal, as they are not duplicated.\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='quantitative'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n)\n\n\n\n\n\n\n\n\n\n\n2.5.3 Heatmap Matrix\nWhen scatter plots become too saturated (many overlapping points), heatmaps offer a better alternative by binning the numeric values and encoding the count in color intensity.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X(alt.repeat('column'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y(alt.repeat('row'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n).resolve_scale(\n    color='independent'\n)\n\n\n\n\n\n\n\n\nTo gain deeper insights into the dataset, it’s important to analyze how numerical variables behave across different categories. This type of multivariate analysis allows us to:\n\nCompare distributions across categories\nDetect outliers within categories\nObserve central tendency (median, quartiles) and spread (range, IQR)\n\nBoxplots are particularly effective for this purpose. In the following visualizations, we explore these relationships by repeating plots across combinations of categorical and numerical features.\n\n\n2.5.4 Filter Categorical Columns\nFirst, we select the relevant categorical columns, excluding identifiers and text-heavy variables like movie titles or director names.\n\n\nCode\ncategorical_columns =  movies_cleaned.select_dtypes('object').columns.to_list()\n\ncategorical_columns_remove = ['Title','Release_Date','Distributor','Director']\n\ncategorical_filtered = [col for col in categorical_columns if col not in categorical_columns_remove]\n\n\n\n\n2.5.5 Repeated Boxplots: Categorical vs Numerical\nWe repeat boxplots using combinations of categorical (rows) and numerical (columns) features. This matrix layout gives a clear visual overview of how numerical values are distributed within each category.\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=numerical_columns,\n    row=categorical_filtered\n)\n\n\n\n\n\n\n\n\n\n\n2.5.6 Faceted Boxplots\nFor more focused analysis, we can facet the boxplots using a specific categorical variable like MPAA_Rating, and repeat the chart by different categorical rows. This lets us keep the numerical axis fixed while comparing how categories vary across different classes (e.g., movie ratings).\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Running_Time_min', type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).facet(\n    column='MPAA_Rating'\n).repeat(\n    row=categorical_filtered\n)",
    "crumbs": [
      "Eda"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PRIMER REPORTE EN QUARTO",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape= \"Origin\",\n    tooltip=[\"Name\", \"Origin\", \"Miles_per_Gallon\", \"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort=\"y\"),\n    alt.Y('count()')\n)\n\n\n\n\n\n\n\n\nMean() de cada uno de los origenes\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort=\"y\"),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width=600\n)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#subtitulo",
    "href": "index.html#subtitulo",
    "title": "PRIMER REPORTE EN QUARTO",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#visualizacion-en-altair",
    "href": "index.html#visualizacion-en-altair",
    "title": "PRIMER REPORTE EN QUARTO",
    "section": "",
    "text": "Code\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    shape= \"Origin\",\n    tooltip=[\"Name\", \"Origin\", \"Miles_per_Gallon\", \"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort=\"y\"),\n    alt.Y('count()')\n)\n\n\n\n\n\n\n\n\nMean() de cada uno de los origenes\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort=\"y\"),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width=200,\n    height=200\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width=600\n)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "laboratorio 2",
    "section": "",
    "text": "Import libraries\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\StarMedia\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\n\nCargar dataset\n\nurl = \"https://raw.githubusercontent.com/erickedu85/dataset/refs/heads/master/tweets/tweets_totales_con_sentimiento_ml.csv\"\ndf = pd.read_csv(url)\nprint(df.shape)\ndf.head()\n\n(158873, 26)\n\n\n\n\n\n\n\n\n\ntweetId\ntweetUrl\ncontent\nisReply\nreplyTo\ncreatedAt\nauthorId\nauthorName\nauthorUsername\nauthorVerified\n...\nconversationId\ninReplyToId\nDate\ntime_response\naccount_age_days\nmentions_count\nhashtags_count\ncontent_length\nhas_profile_picture\nsentiment_polarity\n\n\n\n\n0\n1877190873579950336\nhttps://x.com/hectorjalonm/status/187719087357...\n@DiegoPonguill10 @DanielNoboaOk @LuisaGonzalez...\nTrue\nDiegoPonguill10\n2025-01-09 03:09:00\n1458536175119986688\nhéctor 🌵\nhectorjalonm\nFalse\n...\n1877115691146306005\n1877188297551650816\n2025-01-08 22:10:30\n298.5\n1151\n3\n0\n88\nTrue\n0.0\n\n\n1\n1877188297551650816\nhttps://x.com/DiegoPonguill10/status/187718829...\n@hectorjalonm @DanielNoboaOk @LuisaGonzalezEc ...\nTrue\nhectorjalonm\n2025-01-09 02:59:00\n1555549203211976704\nDiego Ponguillo Vargas TODO TODITO #5 😉 HLVS!!\nDiegoPonguill10\nFalse\n...\n1877115691146306005\n1877123519743451648\n2025-01-08 22:10:30\n288.5\n883\n3\n0\n119\nTrue\n0.0\n\n\n2\n1877186248986501120\nhttps://x.com/ekuador_593/status/1877186248986...\n@Gregori58965636 @yesendiaz @DanielNoboaOk Otr...\nTrue\nGregori58965636\n2025-01-09 02:50:00\n1457524470365708288\nSebastián Noboa Ec\nekuador_593\nFalse\n...\n1877115691146306005\n1877122565782576896\n2025-01-08 22:10:30\n279.5\n1153\n3\n0\n60\nTrue\n0.0\n\n\n3\n1877168335193833472\nhttps://x.com/JRamirez2O24/status/187716833519...\n@jdiegol2010 @DanielNoboaOk https://t.co/CsLWQ...\nTrue\njdiegol2010\n2025-01-09 01:39:00\n1759130630766341888\nANDRES RAMIREZ\nJRamirez2O24\nFalse\n...\n1877115691146306005\n1877159202646315264\n2025-01-08 22:10:30\n208.5\n321\n2\n0\n51\nTrue\n0.0\n\n\n4\n1877159202646315264\nhttps://x.com/jdiegol2010/status/1877159202646...\n@JRamirez2O24 @DanielNoboaOk El tema es respet...\nTrue\nJRamirez2O24\n2025-01-09 01:03:00\n146111157\nJuan Diego\njdiegol2010\nFalse\n...\n1877115691146306005\n1877130145938481152\n2025-01-08 22:10:30\n172.5\n5343\n2\n0\n80\nTrue\n0.0\n\n\n\n\n5 rows × 26 columns\n\n\n\n\n\nLimpieza\n\n# Eliminamos columnas que no aportan valor predictivo (identificadores, URLs, fechas, imágenes, etc.)\ncols_utiles = [\n    'content', 'authorVerified', 'authorFollowers',\n    'hashtags_count', 'mentions_count', 'content_length',\n    'account_age_days'\n]\n\ndf = df[cols_utiles].dropna()\nprint(\"Dimensiones después de limpieza:\", df.shape)\n\n# Limpieza de texto\ndef clean_text(text):\n    text = re.sub(r\"http\\S+\", \"\", text)       # eliminar URLs\n    text = re.sub(r\"@\\w+\", \"\", text)          # eliminar menciones\n    text = re.sub(r\"#\\w+\", \"\", text)          # eliminar hashtags\n    text = re.sub(r\"[^A-Za-zÀ-ÿ\\s]\", \"\", text) # eliminar símbolos\n    text = text.lower().strip()\n    return text\n\ndf['content_clean'] = df['content'].apply(clean_text)\n\ndf.head()\n\nDimensiones después de limpieza: (158873, 7)\n\n\n\n\n\n\n\n\n\ncontent\nauthorVerified\nauthorFollowers\nhashtags_count\nmentions_count\ncontent_length\naccount_age_days\ncontent_clean\n\n\n\n\n0\n@DiegoPonguill10 @DanielNoboaOk @LuisaGonzalez...\nFalse\n145\n0\n3\n88\n1151\njajajaajajajajajajajajjajaajja okkkkkkk\n\n\n1\n@hectorjalonm @DanielNoboaOk @LuisaGonzalezEc ...\nFalse\n176\n0\n3\n119\n883\nahora vivimos en la miseria antes fuimos el me...\n\n\n2\n@Gregori58965636 @yesendiaz @DanielNoboaOk Otr...\nFalse\n147\n0\n3\n60\n1153\notro troll basura\n\n\n3\n@jdiegol2010 @DanielNoboaOk https://t.co/CsLWQ...\nFalse\n87\n0\n2\n51\n321\n\n\n\n4\n@JRamirez2O24 @DanielNoboaOk El tema es respet...\nFalse\n100\n0\n2\n80\n5343\nel tema es respetar a quien eligió el pueblo o no\n\n\n\n\n\n\n\nElimine variables que no aportan información analítica (IDs, fechas, etc.).\nElimine gran parte del ruido como “@,#,links”, manteniendo solo palabras que pueden aportar valor semántico.\n\n\nSelección del target\n\ndf['tipo_usuario'] = np.where(\n    (df['authorFollowers'] &gt; 1000) | (df['authorVerified'] == True),\n    'Influencer',\n    'Regular'\n)\n\nPrimero verifico el tipo de usuario, como influencer o regular sgun el numero de seguidores que tiene\n\n# Creamos un puntaje ponderado de interacción\ndf['interaccion_score'] = (\n    df['mentions_count']*0.4 +\n    df['hashtags_count']*0.3 +\n    df['content_length']*0.2 +\n    np.log1p(df['authorFollowers'])*0.1\n)\n\n# Definimos nivel alto o bajo según percentil 75\numbral = df['interaccion_score'].quantile(0.75)\ndf['nivel_interaccion'] = np.where(df['interaccion_score'] &gt;= umbral, 'Alta', 'Baja')\n\nObtengo un porcentaje del tipo de interacciones que suele tener el usuario, esto para saber un aproximado de cuanta actividad constante suele tener\n\ndef crear_perfil(row):\n    if row['tipo_usuario'] == 'Influencer' and row['nivel_interaccion'] == 'Alta':\n        return 'Influyente Activo'\n    elif row['tipo_usuario'] == 'Influencer' and row['nivel_interaccion'] == 'Baja':\n        return 'Influyente Pasivo'\n    elif row['tipo_usuario'] == 'Regular' and row['nivel_interaccion'] == 'Alta':\n        return 'Usuario Participativo'\n    else:\n        return 'Usuario Pasivo'\n\ndf['perfil_usuario'] = df.apply(crear_perfil, axis=1)\n\ndf['perfil_usuario'].value_counts()\n\nperfil_usuario\nUsuario Pasivo           111449\nUsuario Participativo     36881\nInfluyente Pasivo          7705\nInfluyente Activo          2838\nName: count, dtype: int64\n\n\nEl target perfil_usuario tiene 4 categorías que reflejan diferentes comportamientos comunicativos en la red:\n\nInfluyente Activo: usuarios con alto alcance y actividad alta.\nInfluyente Pasivo: usuarios con alcance alto pero poca interacción.\nUsuario Participativo: personas comunes con mucha actividad.\nUsuario Pasivo: cuentas regulares con baja actividad.\n\n\n\nSelección de features relevantes\n\nfeatures = [\n    'content_clean',          # texto procesado\n    'authorVerified',\n    'authorFollowers',\n    'hashtags_count',\n    'mentions_count',\n    'content_length',\n    'account_age_days'\n]\n\ntarget = 'perfil_usuario'\n\nX = df[features]\nX[\"authorVerified\"] = X[\"authorVerified\"].astype(int)\ny = df[target]\n\nC:\\Users\\StarMedia\\AppData\\Local\\Temp\\ipykernel_16228\\3166960640.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X[\"authorVerified\"] = X[\"authorVerified\"].astype(int)\n\n\n\n\nPREPROCESAMIENTO DEL TEXTO (TF-IDF)\n\ntext_features = [\"content_clean\"]\nnumeric_features = [\"authorFollowers\", \"hashtags_count\", \"mentions_count\", \"content_length\", \"account_age_days\"]\ncategorical_features = [\"authorVerified\"]\n\nspanish_stopwords = stopwords.words('spanish')\ntext_transformer = TfidfVectorizer(stop_words=spanish_stopwords)\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\nDividi los features relevantes del dataset en 3 tipos de variables: textuales, numericas y categoricas.\nGenere los 3 tipos de transformes para poder convertir los valores del dataset a datos que el modelo pueda entender.\n\n\nPREPROCESAMIENTO Y MODELADO\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"text\", text_transformer, \"content_clean\"),\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features)\n    ]\n)\n\n# Modelo base (simple y eficiente)\n\n\n\nPipeline\n\nmodel = LogisticRegression(max_iter=300, multi_class=\"multinomial\")\n\npipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"classifier\", model)\n])\n\nUse LogisticRegression que permite combinar de manera eficiente texto vectorizado, variables numéricas y categóricas dentro de un mismo pipeline\nAplique TF-IDF obtener una matriz numerica que permita facilitar el analisis de los valores para mi modelo\nUse OneHotEncoder para convertir las variables categórica en columnas binarias\nUse StandardScaler para normalizar las diferentes escalas y que todas tengan el mismo peso en el modelo.\n\n\nDividir los datos\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\n\n\nFit\n\npipeline.fit(X_train, y_train)\n\nc:\\respaldo para reparacion de disco\\RESPALDO\\StarMedia\\desarrollo\\Maestria Inteligencia artificial\\Clase Inicial\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('text',\n                                                  TfidfVectorizer(stop_words=['de',\n                                                                              'la',\n                                                                              'que',\n                                                                              'el',\n                                                                              'en',\n                                                                              'y',\n                                                                              'a',\n                                                                              'los',\n                                                                              'del',\n                                                                              'se',\n                                                                              'las',\n                                                                              'por',\n                                                                              'un',\n                                                                              'para',\n                                                                              'con',\n                                                                              'no',\n                                                                              'una',\n                                                                              'su',\n                                                                              'al',\n                                                                              'lo',\n                                                                              'como',\n                                                                              'más',\n                                                                              'pero',\n                                                                              'sus',\n                                                                              'le',\n                                                                              'ya',\n                                                                              'o',\n                                                                              'este',\n                                                                              'sí',\n                                                                              'porque', ...]),\n                                                  'content_clean'),\n                                                 ('num', StandardScaler(),\n                                                  ['authorFollowers',\n                                                   'hashtags_count',\n                                                   'mentions_count',\n                                                   'content_length',\n                                                   'account_age_days']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['authorVerified'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=300, multi_class='multinomial'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('preprocessor', ...), ('classifier', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('text', ...), ('num', ...), ...]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    textcontent_cleanTfidfVectorizer?Documentation for TfidfVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ninput \n'content'\n\n\n\nencoding \n'utf-8'\n\n\n\ndecode_error \n'strict'\n\n\n\nstrip_accents \nNone\n\n\n\nlowercase \nTrue\n\n\n\npreprocessor \nNone\n\n\n\ntokenizer \nNone\n\n\n\nanalyzer \n'word'\n\n\n\nstop_words \n['de', 'la', ...]\n\n\n\ntoken_pattern \n'(?u)\\\\b\\\\w\\\\w+\\\\b'\n\n\n\nngram_range \n(1, ...)\n\n\n\nmax_df \n1.0\n\n\n\nmin_df \n1\n\n\n\nmax_features \nNone\n\n\n\nvocabulary \nNone\n\n\n\nbinary \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nnorm \n'l2'\n\n\n\nuse_idf \nTrue\n\n\n\nsmooth_idf \nTrue\n\n\n\nsublinear_tf \nFalse\n\n\n\n\n            \n        \n    num['authorFollowers', 'hashtags_count', 'mentions_count', 'content_length', 'account_age_days']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    cat['authorVerified']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n300\n\n\n\nmulti_class \n'multinomial'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nPredict\n\ny_pred = pipeline.predict(X_test)\n\n\n\nEVALUACIÓN DEL MODELO\n\nprint(\"📊 Reporte de Clasificación:\\n\")\nprint(classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\ndisp.plot(cmap=\"Blues\", xticks_rotation=45)\nplt.title(\"Matriz de Confusión - Clasificación de Perfiles de Usuario\")\nplt.show()\n\n📊 Reporte de Clasificación:\n\n                       precision    recall  f1-score   support\n\n    Influyente Activo       0.99      0.21      0.34       710\n    Influyente Pasivo       1.00      0.21      0.35      1926\nUsuario Participativo       0.94      1.00      0.97      9220\n       Usuario Pasivo       0.95      1.00      0.97     27863\n\n             accuracy                           0.95     39719\n            macro avg       0.97      0.60      0.66     39719\n         weighted avg       0.95      0.95      0.93     39719\n\n\n\n\n\n\n\n\n\n\n\n# Comparar distribuciones reales y predichas\nfig, ax = plt.subplots(1, 2, figsize=(12,5))\n\nsns.countplot(y_test, ax=ax[0], palette=\"pastel\")\nax[0].set_title(\"Distribución Real de Clases\")\nax[0].set_xlabel(\"Perfil de Usuario\")\nax[0].set_ylabel(\"Cantidad\")\n\nsns.countplot(y_pred, ax=ax[1], palette=\"pastel\")\nax[1].set_title(\"Distribución Predicha por el Modelo\")\nax[1].set_xlabel(\"Perfil de Usuario\")\nax[1].set_ylabel(\"Cantidad\")\n\nplt.tight_layout()\nplt.show()\n\nC:\\Users\\StarMedia\\AppData\\Local\\Temp\\ipykernel_16228\\1990516036.py:4: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(y_test, ax=ax[0], palette=\"pastel\")\nC:\\Users\\StarMedia\\AppData\\Local\\Temp\\ipykernel_16228\\1990516036.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(y_pred, ax=ax[1], palette=\"pastel\")",
    "crumbs": [
      "Laboratorios",
      "laboratorio 2"
    ]
  },
  {
    "objectID": "Pfinal.html",
    "href": "Pfinal.html",
    "title": "Proyecto Final",
    "section": "",
    "text": "Code\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score,\n                             f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay,\n                             mean_absolute_error, mean_squared_error, r2_score)\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.pipeline import make_pipeline\n\nRANDOM_STATE = 42\n\nimport nltk\nfrom nltk.corpus import stopwords\nsns.set(style=\"whitegrid\")\nnltk.download('stopwords')\nstop_words = set(stopwords.words('spanish'))\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\StarMedia\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\n\n\n\n\nCode\ndf = pd.read_csv('1500_tweets_con_toxicity.csv')\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 27 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   tweetId              1500 non-null   int64  \n 1   tweetUrl             1500 non-null   object \n 2   content              1500 non-null   object \n 3   isReply              1500 non-null   bool   \n 4   replyTo              1490 non-null   object \n 5   createdAt            1500 non-null   object \n 6   authorId             1500 non-null   int64  \n 7   authorName           1500 non-null   object \n 8   authorUsername       1500 non-null   object \n 9   authorVerified       1500 non-null   bool   \n 10  authorFollowers      1500 non-null   int64  \n 11  authorProfilePic     1500 non-null   object \n 12  authorJoinDate       1500 non-null   object \n 13  source               1500 non-null   object \n 14  hashtags             121 non-null    object \n 15  mentions             1499 non-null   object \n 16  conversationId       1500 non-null   int64  \n 17  inReplyToId          1500 non-null   int64  \n 18  Date                 1500 non-null   object \n 19  time_response        1500 non-null   float64\n 20  account_age_days     1500 non-null   int64  \n 21  mentions_count       1500 non-null   int64  \n 22  hashtags_count       1500 non-null   int64  \n 23  content_length       1500 non-null   int64  \n 24  has_profile_picture  1500 non-null   bool   \n 25  sentiment_polarity   1500 non-null   float64\n 26  toxicity_score       1347 non-null   float64\ndtypes: bool(3), float64(3), int64(9), object(12)\nmemory usage: 285.8+ KB\n\n\n\n\n\n\n\n\n\ntweetId\ntweetUrl\ncontent\nisReply\nreplyTo\ncreatedAt\nauthorId\nauthorName\nauthorUsername\nauthorVerified\n...\ninReplyToId\nDate\ntime_response\naccount_age_days\nmentions_count\nhashtags_count\ncontent_length\nhas_profile_picture\nsentiment_polarity\ntoxicity_score\n\n\n\n\n0\n1878630970745900800\nhttps://x.com/Pableins15/status/18786309707459...\n@DanielNoboaOk @DiegoBorjaPC Lávate el hocico ...\nTrue\nDanielNoboaOk\n2025-01-13 02:31:00\n176948611\nPablo Balarezo\nPableins15\nFalse\n...\n1878539079249547520\n2025-01-12 20:26:32\n364.466667\n5261\n2\n0\n309\nFalse\n0.0\n0.543256\n\n\n1\n1904041877503984128\nhttps://x.com/solma1201/status/190404187750398...\n@DanielNoboaOk De esa arrastrada no te levanta...\nTrue\nDanielNoboaOk\n2025-03-24 05:25:00\n1368663286582030336\nSolma1201\nsolma1201\nFalse\n...\n1904003201143115776\n2025-03-24 02:51:52\n153.133333\n1399\n1\n0\n70\nTrue\n0.0\n0.426917\n\n\n2\n1877463444649046016\nhttps://x.com/Mediterran67794/status/187746344...\n@LuisaGonzalezEc @RC5Oficial Protegiendo a los...\nTrue\nLuisaGonzalezEc\n2025-01-09 21:12:00\n1851005619106451712\nMédico Escritor Filósofo Hermeneútico\nMediterran67794\nFalse\n...\n1877158437236228352\n2025-01-09 01:00:22\n1211.633333\n68\n2\n0\n122\nTrue\n0.0\n0.555970\n\n\n3\n1881356046108885248\nhttps://x.com/ardededa/status/1881356046108885494\n@DanielNoboaOk #NoboaPresidente. Todo 7!\nTrue\nDanielNoboaOk\n2025-01-20 15:00:00\n315799544\nDenise\nardededa\nFalse\n...\n1881165128185560832\n2025-01-20 02:21:31\n758.483333\n4955\n1\n0\n41\nTrue\n0.0\n0.046615\n\n\n4\n1888331962063978752\nhttps://x.com/LMarquinezm/status/1888331962063...\n@slider1908 @LuisaGonzalezEc @DianaAtamaint @c...\nTrue\nslider1908\n2025-02-08 20:59:00\n1551883554\nLuis Marquínez\nLMarquinezm\nFalse\n...\n1888256000085397504\n2025-02-08 14:59:07\n359.883333\n4208\n5\n0\n101\nTrue\n0.0\n0.846027\n\n\n\n\n5 rows × 27 columns\n\n\n\n\n\nCode\nif 'tweetId' in df.columns:\n    df.drop_duplicates(subset='tweetId', inplace=True)\n\n# Revisar nulos por columna\nprint(\"Nulos por columna:\")\nprint(df.isnull().sum().sort_values(ascending=False).head(20))\n\n\nNulos por columna:\nhashtags            1379\ntoxicity_score       153\nreplyTo               10\nmentions               1\ncontent                0\ncreatedAt              0\nauthorId               0\nauthorName             0\nisReply                0\ntweetUrl               0\ntweetId                0\nauthorFollowers        0\nauthorVerified         0\nauthorUsername         0\nauthorJoinDate         0\nsource                 0\nconversationId         0\ninReplyToId            0\nauthorProfilePic       0\nDate                   0\ndtype: int64\n\n\nEn esta sección se realiza una inspección inicial de la estructura del conjunto de datos y se identifican los valores faltantes (nulos), puesto a que se sabe que el dataset tiene 1500 datos.\n\n\nPara mantener el control de los datos de la columna targe, que sera la columna toxicity_score, primero es necesario eliminar los valores nulos, puesto que la api, en ocaciones puede fallar, dejando filas sin este valor, dificultando su procesamiento dentro del modelo\n\n\nCode\ndf = df.dropna(subset=['toxicity_score'])\ndf.isnull().sum().sort_values(ascending=False)\n\n\nhashtags               1229\nreplyTo                   9\nmentions                  1\ncontent                   0\nisReply                   0\ncreatedAt                 0\nauthorId                  0\nauthorName                0\nauthorUsername            0\ntweetUrl                  0\ntweetId                   0\nauthorFollowers           0\nauthorVerified            0\nauthorJoinDate            0\nauthorProfilePic          0\nsource                    0\nconversationId            0\ninReplyToId               0\nDate                      0\ntime_response             0\naccount_age_days          0\nmentions_count            0\nhashtags_count            0\ncontent_length            0\nhas_profile_picture       0\nsentiment_polarity        0\ntoxicity_score            0\ndtype: int64\n\n\nAqui se puede observar como ya no existen valores nulos en la columna toxicity_score\n\n\n\n\n\nCode\ncolumns_to_keep = [\n    'content', 'isReply', 'authorVerified', 'has_profile_picture',\n    'authorFollowers', 'account_age_days', 'mentions_count',\n    'hashtags_count', 'content_length', 'sentiment_polarity',\n    'toxicity_score'\n]\ndf = df[columns_to_keep]\n\n\n\n\n\n\n\nCode\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub(r\"http\\S+\", \"\", text)         # URLs\n    text = re.sub(r\"@\\w+\", \"\", text)            # menciones\n    text = re.sub(r\"#\\w+\", \"\", text)            # hashtags\n    text = re.sub(r\"[^a-záéíóúñü\\s]\", \"\", text) # puntuación\n    text = \" \".join([word for word in text.split() if word not in stop_words])\n    return text\n\ndf['clean_content'] = df['content'].apply(clean_text)\n\n\nPara permitirme una mejor limpieza de los textos hice uso de stopwords, y para este caso en especifico de las stopwords que permite usar el nltk del idioma español\n\n\n\n\n\n\nDespués de eliminar los valores nulos dentro de la columna objetivo, se puede observar que quedaron 1347 registros para su procesamiento\n\n\nCode\n# Estadísticas numéricas\nprint(df.describe().T)\n\nplt.figure(figsize=(8,4))\nsns.histplot(df['toxicity_score'], bins=30, kde=True)\nplt.title('Distribución de TOXICITY_SCORE')\nplt.xlabel('Toxicity score (0-1)')\nplt.show()\n\n\n                     count         mean            std       min         25%  \\\nauthorFollowers     1347.0  4001.405345  124989.608109   0.00000    6.500000   \naccount_age_days    1347.0  2233.675575    1980.549465 -90.00000  447.000000   \nmentions_count      1347.0     1.747587       0.960660   0.00000    1.000000   \nhashtags_count      1347.0     0.000000       0.000000   0.00000    0.000000   \ncontent_length      1347.0   124.263549      77.691218  20.00000   66.000000   \nsentiment_polarity  1347.0    -0.008434       0.122688  -1.00000    0.000000   \ntoxicity_score      1347.0     0.253879       0.243942   0.00194    0.028444   \n\n                            50%          75%           max  \nauthorFollowers       39.000000   186.000000  4.577730e+06  \naccount_age_days    1438.000000  4372.000000  6.506000e+03  \nmentions_count         2.000000     2.000000  1.000000e+01  \nhashtags_count         0.000000     0.000000  0.000000e+00  \ncontent_length       102.000000   158.500000  6.840000e+02  \nsentiment_polarity     0.000000     0.000000  1.000000e+00  \ntoxicity_score         0.188392     0.426917  9.391453e-01  \n\n\n\n\n\n\n\n\n\nEste grafico nos da a entender como es la distribucion de la cantidad de toxicidad en los registros, mostrando que por lo general se evita tener una gran toxicidad\n\n\n\n\n\nCode\nplt.figure(figsize=(6,3))\nsns.boxplot(x=df['toxicity_score'])\nplt.title('Boxplot TOXICITY_SCORE')\nplt.show()\n\n\n\n\n\n\n\n\n\nEl boxplot se observa una dispersión limitada y muy pocos outliers dejando a la vista que escoger un valor mayor a 0.5 será lo mejor para poder seleccionar si un comentario es toxico o no\n\n\n\n\n\nCode\nif 'isReply' in df.columns:\n    plt.figure(figsize=(6,3))\n    sns.countplot(x='isReply', data=df)\n    plt.title('isReply distribución')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nTras observar la distribución de toxicity_score (histograma y boxplot), se decide usar umbral = 0.6 como punto incial para considerar ‘tóxico’ porque suele marcar la zona donde la densidad decrece significativamente hacia valores altos.\n\n\nCode\nthreshold = 0.6\ndf['target_toxic'] = (df['toxicity_score'] &gt;= threshold).astype(int)\n\n# Ver la proporción de clases\nprint(\"Distribución binaria (0=no tóxico, 1=tóxico):\")\nprint(df['target_toxic'].value_counts(normalize=True))\nsns.countplot(x='target_toxic', data=df)\nplt.title(f'Distribución target con umbral = {threshold}')\nplt.show()\n\n\nDistribución binaria (0=no tóxico, 1=tóxico):\ntarget_toxic\n0    0.888641\n1    0.111359\nName: proportion, dtype: float64\n\n\n\n\n\n\n\n\n\nEl gráfico de barras evidencia un fuerte desbalance: aproximadamente 89% no tóxicos y 11% tóxicos. Con esto se avanzo para realizar el procesamiento de los datos.\n\n\n\n\n\n\n\n\nCode\nfeatures = [\n    'clean_content',          # texto procesado\n    'authorVerified',\n    'has_profile_picture',\n    'isReply',\n    'authorFollowers',\n    'account_age_days',\n    'mentions_count',\n    'hashtags_count', \n    'content_length', \n    'sentiment_polarity'\n]\n\ntext_feature = ['clean_content']\ncategorical_features = ['isReply', 'authorVerified', 'has_profile_picture']\nnumeric_features = [\n    'authorFollowers', 'account_age_days', 'mentions_count',\n    'hashtags_count', 'content_length', 'sentiment_polarity'\n]\nprint(['clean_content'])\n\n\n['clean_content']\n\n\n\n\n\n\n\nCode\nspanish_stopwords = stopwords.words('spanish')\ntfidf = TfidfVectorizer(stop_words=spanish_stopwords)\ncat_encoder = OneHotEncoder(handle_unknown='ignore')\nscaler = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', tfidf, 'clean_content'),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n        ('num', StandardScaler(), numeric_features)\n    ],\n    remainder='drop'\n)\n\n\nSe aplicaron transformaciones con TF-IDF para texto, escalado para variables numéricas y codificación one-hot para categóricas, integradas en un ColumnTransformer.\n\n\n\n\n\nCode\nX = df[['clean_content'] + categorical_features + numeric_features]\ny_class = df['target_toxic']        # para clasificación binaria\ny_reg = df['toxicity_score']        # para regresión (continuo)\n\nX_train, X_test, y_train_cl, y_test_cl = train_test_split(\n    X, y_class, test_size=0.2, random_state=RANDOM_STATE, stratify=y_class\n)\n\n# Para regresión (mismo split de X, pero con y_reg alineado):\n# usamos el mismo índice de train/test para evitar fugas:\n_, _, y_train_reg, y_test_reg = train_test_split(\n    X, y_reg, test_size=0.2, random_state=RANDOM_STATE, stratify=y_class\n)\n\n\n\n\n\n\nPara la clasificación se entrenó un modelo de regresión logística. Esto debido a que el tipo de datos que se puede buscar es un binario. 1 es toxico, 2 no lo es.\n\n\n\n\nCode\nclf_logistic = Pipeline([\n    ('preproc', preprocessor),\n    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', random_state=RANDOM_STATE))\n])\n\n# Entrenar\nclf_logistic.fit(X_train, y_train_cl)\n\n\nPipeline(steps=[('preproc',\n                 ColumnTransformer(transformers=[('text',\n                                                  TfidfVectorizer(stop_words=['de',\n                                                                              'la',\n                                                                              'que',\n                                                                              'el',\n                                                                              'en',\n                                                                              'y',\n                                                                              'a',\n                                                                              'los',\n                                                                              'del',\n                                                                              'se',\n                                                                              'las',\n                                                                              'por',\n                                                                              'un',\n                                                                              'para',\n                                                                              'con',\n                                                                              'no',\n                                                                              'una',\n                                                                              'su',\n                                                                              'al',\n                                                                              'lo',\n                                                                              'como',\n                                                                              'más',\n                                                                              'pero',\n                                                                              'sus',\n                                                                              'le',\n                                                                              'ya',\n                                                                              'o',\n                                                                              'este',\n                                                                              'sí',\n                                                                              'porque', ...]),\n                                                  'clean_content'),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['isReply', 'authorVerified',\n                                                   'has_profile_picture']),\n                                                 ('num', StandardScaler(),\n                                                  ['authorFollowers',\n                                                   'account_age_days',\n                                                   'mentions_count',\n                                                   'hashtags_count',\n                                                   'content_length',\n                                                   'sentiment_polarity'])])),\n                ('clf',\n                 LogisticRegression(class_weight='balanced', max_iter=2000,\n                                    random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('preproc', ...), ('clf', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    preproc: ColumnTransformer?Documentation for preproc: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('text', ...), ('cat', ...), ...]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    textclean_contentTfidfVectorizer?Documentation for TfidfVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ninput \n'content'\n\n\n\nencoding \n'utf-8'\n\n\n\ndecode_error \n'strict'\n\n\n\nstrip_accents \nNone\n\n\n\nlowercase \nTrue\n\n\n\npreprocessor \nNone\n\n\n\ntokenizer \nNone\n\n\n\nanalyzer \n'word'\n\n\n\nstop_words \n['de', 'la', ...]\n\n\n\ntoken_pattern \n'(?u)\\\\b\\\\w\\\\w+\\\\b'\n\n\n\nngram_range \n(1, ...)\n\n\n\nmax_df \n1.0\n\n\n\nmin_df \n1\n\n\n\nmax_features \nNone\n\n\n\nvocabulary \nNone\n\n\n\nbinary \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nnorm \n'l2'\n\n\n\nuse_idf \nTrue\n\n\n\nsmooth_idf \nTrue\n\n\n\nsublinear_tf \nFalse\n\n\n\n\n            \n        \n    cat['isReply', 'authorVerified', 'has_profile_picture']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    num['authorFollowers', 'account_age_days', 'mentions_count', 'hashtags_count', 'content_length', 'sentiment_polarity']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \n'balanced'\n\n\n\nrandom_state \n42\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n2000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_pred_log = clf_logistic.predict(X_test)\ny_proba_log = clf_logistic.predict_proba(X_test)[:,1]\n\nprint(\"Logistic Regression:\")\nprint(\"Accuracy:\", accuracy_score(y_test_cl, y_pred_log))\nprint(\"Precision:\", precision_score(y_test_cl, y_pred_log, zero_division=0))\nprint(\"Recall:\", recall_score(y_test_cl, y_pred_log, zero_division=0))\nprint(\"F1:\", f1_score(y_test_cl, y_pred_log, zero_division=0))\nprint(\"ROC-AUC:\", roc_auc_score(y_test_cl, y_proba_log))\n\n\nLogistic Regression:\nAccuracy: 0.8518518518518519\nPrecision: 0.3333333333333333\nRecall: 0.3333333333333333\nF1: 0.3333333333333333\nROC-AUC: 0.7061111111111111\n\n\nSe obtuvo una precisión aceptable (~85%) pero con métricas de recall y F1 moderadas debido al desbalance de clases, ya que existe en su gran mayoría un mayor número de comentarios no tóxicos, frente a los tóxicos.\n\n\n\n\n\nCode\ncm = confusion_matrix(y_test_cl, y_pred_log)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no_tox','tox'])\ndisp.plot(cmap='Blues')\nplt.title('Confusion Matrix - Logistic')\nplt.show()\n\n\n\n\n\n\n\n\n\nMuestra una buena detección de la clase no tóxica, aunque algunos falsos negativos persisten.\n\n\n\n\n\nCode\nRocCurveDisplay.from_estimator(clf_logistic, X_test, y_test_cl)\nplt.title('Logistic')\nplt.show()\n\n\n\n\n\n\n\n\n\nEl área bajo la curva (ROC-AUC ≈ 0.70) refleja un rendimiento medio, con capacidad moderada de distinguir entre tweets tóxicos y no tóxicos.\n\n\n\n\nSe implementaron modelos LinearRegression y RandomForestRegressor para predecir la toxicidad continua. El Random Forest mostró mejor ajuste (menor MAE y mayor R²). Se utilizaron 2 modelos para poder evaluar si la toxicidad puede explicarse de forma lineal o si requiere un enfoque más flexible. En este caso, el Random Forest mostró mejor rendimiento (menor error y mayor R²), lo que sugiere relaciones más complejas en los datos.\n\n\n\n\nCode\nreg_lin = Pipeline([\n    ('preproc', preprocessor),\n    ('linreg', LinearRegression())\n])\n\nreg_rf = Pipeline([\n    ('preproc', preprocessor),\n    ('rf', RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE))\n])\n\n# Entrenar\nreg_lin.fit(X_train, y_train_reg)\nreg_rf.fit(X_train, y_train_reg)\n\n\nPipeline(steps=[('preproc',\n                 ColumnTransformer(transformers=[('text',\n                                                  TfidfVectorizer(stop_words=['de',\n                                                                              'la',\n                                                                              'que',\n                                                                              'el',\n                                                                              'en',\n                                                                              'y',\n                                                                              'a',\n                                                                              'los',\n                                                                              'del',\n                                                                              'se',\n                                                                              'las',\n                                                                              'por',\n                                                                              'un',\n                                                                              'para',\n                                                                              'con',\n                                                                              'no',\n                                                                              'una',\n                                                                              'su',\n                                                                              'al',\n                                                                              'lo',\n                                                                              'como',\n                                                                              'más',\n                                                                              'pero',\n                                                                              'sus',\n                                                                              'le',\n                                                                              'ya',\n                                                                              'o',\n                                                                              'este',\n                                                                              'sí',\n                                                                              'porque', ...]),\n                                                  'clean_content'),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['isReply', 'authorVerified',\n                                                   'has_profile_picture']),\n                                                 ('num', StandardScaler(),\n                                                  ['authorFollowers',\n                                                   'account_age_days',\n                                                   'mentions_count',\n                                                   'hashtags_count',\n                                                   'content_length',\n                                                   'sentiment_polarity'])])),\n                ('rf',\n                 RandomForestRegressor(n_estimators=200, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('preproc', ...), ('rf', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    preproc: ColumnTransformer?Documentation for preproc: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('text', ...), ('cat', ...), ...]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    textclean_contentTfidfVectorizer?Documentation for TfidfVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ninput \n'content'\n\n\n\nencoding \n'utf-8'\n\n\n\ndecode_error \n'strict'\n\n\n\nstrip_accents \nNone\n\n\n\nlowercase \nTrue\n\n\n\npreprocessor \nNone\n\n\n\ntokenizer \nNone\n\n\n\nanalyzer \n'word'\n\n\n\nstop_words \n['de', 'la', ...]\n\n\n\ntoken_pattern \n'(?u)\\\\b\\\\w\\\\w+\\\\b'\n\n\n\nngram_range \n(1, ...)\n\n\n\nmax_df \n1.0\n\n\n\nmin_df \n1\n\n\n\nmax_features \nNone\n\n\n\nvocabulary \nNone\n\n\n\nbinary \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nnorm \n'l2'\n\n\n\nuse_idf \nTrue\n\n\n\nsmooth_idf \nTrue\n\n\n\nsublinear_tf \nFalse\n\n\n\n\n            \n        \n    cat['isReply', 'authorVerified', 'has_profile_picture']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    num['authorFollowers', 'account_age_days', 'mentions_count', 'hashtags_count', 'content_length', 'sentiment_polarity']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    RandomForestRegressor?Documentation for RandomForestRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n200\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n1.0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n42\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_pred_lin = reg_lin.predict(X_test)\ny_pred_rf = reg_rf.predict(X_test)\n\n# Métricas\ndef regression_metrics(y_true, y_pred, label=\"Model\"):\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    print(f\"{label} -&gt; MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n\nregression_metrics(y_test_reg, y_pred_lin, \"LinearRegression\")\n\n\nLinearRegression -&gt; MAE: 0.1478, RMSE: 0.0407, R2: 0.3225\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(6,6))\nplt.scatter(y_test_reg, y_pred_rf, alpha=0.5)\nplt.plot([0,1],[0,1], 'r--')  # línea ideal\nplt.xlabel(\"Toxicity real\")\nplt.ylabel(\"Toxicity predicho\")\nplt.title(\"Real vs Predicho - RandomForestRegressor\")\nplt.show()\n\n\n\n\n\n\n\n\n\nEl scatter muestra que la mayoría de predicciones del Random Forest siguen la línea ideal, indicando un buen ajuste general.\n\n\n\n\n\nCode\nresid = y_test_reg - y_pred_rf\nplt.figure(figsize=(6,4))\nsns.histplot(resid, bins=30, kde=True)\nplt.title(\"Errores residuales - RandomForestRegressor\")\nplt.xlabel(\"Residual (real - predicho)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLos residuos se concentran cerca de cero, lo que sugiere ausencia de sesgos importantes en las predicciones del modelo.\n\n\n\n\n\n\n\n\nCode\n# 1. Procesar texto con TF-IDF\ntfidf_clust = TfidfVectorizer(max_features=500, stop_words=spanish_stopwords, ngram_range=(1,2))\nX_text_tfidf = tfidf_clust.fit_transform(df['clean_content'])\n\nprint(f\"TF-IDF shape: {X_text_tfidf.shape}\")\n\n# 2. Reducir dimensionalidad del texto con SVD (manejar caso de pocas features)\nn_components = min(20, X_text_tfidf.shape[1] - 1)\nif n_components &lt; 2:\n    n_components = 1\n    print(\" Pocas características en TF-IDF, usando 1 componente\")\nelse:\n    print(f\"Usando {n_components} componentes para SVD\")\n\nsvd = TruncatedSVD(n_components=n_components, random_state=RANDOM_STATE)\nX_text_reduced = svd.fit_transform(X_text_tfidf)\nprint(f\"Texto reducido: {X_text_reduced.shape}\")\n\n# 3. Procesar variables categóricas\ncat_encoder_clust = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nX_cat_encoded = cat_encoder_clust.fit_transform(df[categorical_features])\nprint(f\"Categóricas: {X_cat_encoded.shape}\")\n\n# 4. Estandarizar variables numéricas\nscaler_clust = StandardScaler()\nX_num_scaled = scaler_clust.fit_transform(df[numeric_features])\nprint(f\"Numéricas: {X_num_scaled.shape}\")\n\n# 5. Combinar todas las características\nX_clust = np.hstack([X_text_reduced, X_cat_encoded, X_num_scaled])\nprint(f\"Forma final para clustering: {X_clust.shape}\")\n\n\nTF-IDF shape: (1347, 500)\nUsando 20 componentes para SVD\nTexto reducido: (1347, 20)\nCategóricas: (1347, 4)\nNuméricas: (1347, 6)\nForma final para clustering: (1347, 30)\n\n\nSe usó TF-IDF + SVD para reducir texto y se combinaron con variables categóricas y numéricas.\n\n\n\n\n\nCode\nsil_scores = {}\nfor k in range(2, 7):\n    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n    labels = kmeans.fit_predict(X_clust)\n    sil = silhouette_score(X_clust, labels)\n    sil_scores[k] = sil\n    print(f\"k={k}, silhouette={sil:.4f}\")\n\nbest_k = max(sil_scores, key=sil_scores.get)\nprint(f\"Mejor k por silhouette: {best_k}\")\n\n\nk=2, silhouette=0.9320\nk=3, silhouette=0.2795\nk=4, silhouette=0.3047\nk=5, silhouette=0.3333\nk=6, silhouette=0.3479\nMejor k por silhouette: 2\n\n\nSe probaron valores de k entre 2 y 6; el mejor fue k = 2 (silhouette = 0.93).\n\n\n\n\n\nCode\nkmeans_final = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=10)\ncluster_labels = kmeans_final.fit_predict(X_clust)\n\n# Añadir etiquetas de cluster al dataframe\ndf_clustering = df.copy()\ndf_clustering['cluster_kmeans'] = cluster_labels\n\n\n\n\n\n\n\nCode\n# Reducir a 2D para visualización\npca = PCA(n_components=2, random_state=RANDOM_STATE)\nX_2d = pca.fit_transform(X_clust)\n\nplt.figure(figsize=(15, 5))\n\n# Subplot 1: Clusters de KMeans\nplt.subplot(1, 3, 1)\nscatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, cmap='tab10', alpha=0.6, s=30)\nplt.colorbar(scatter)\nplt.title(f'KMeans Clusters (k={best_k})')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\n\n# Subplot 2: Target binario (toxicidad)\nplt.subplot(1, 3, 2)\ncolors = ['blue', 'red']  # 0=no tóxico, 1=tóxico\nfor target_val in [0, 1]:\n    mask = df_clustering['target_toxic'] == target_val\n    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], \n                c=colors[target_val], label=f'Toxicity={target_val}', alpha=0.6, s=30)\nplt.legend()\nplt.title('Distribución por Toxicidad')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\n\n# Subplot 3: Comparación clusters vs toxicidad\nplt.subplot(1, 3, 3)\nfor cluster_id in range(best_k):\n    mask = df_clustering['cluster_kmeans'] == cluster_id\n    toxic_ratio = df_clustering[mask]['target_toxic'].mean()\n    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], \n                c=['red' if toxic_ratio &gt; 0.5 else 'blue'], \n                label=f'Cluster {cluster_id}', alpha=0.6, s=30)\nplt.title('Clusters coloreados por toxicidad predominante')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLos gráficos PCA en 2D muestran:\n\nDistribución clara entre los dos clusters (izquierda).\nSeparación moderada entre tweets tóxicos y no tóxicos (centro).\nDiferencias visibles en toxicidad promedio por cluster (derecha).\n\n\n\n\n\n\nCode\nprint(\"\\n--- ANÁLISIS CLUSTERS vs TARGET ---\")\n\n# Tabla de contingencia\nct = pd.crosstab(df_clustering['cluster_kmeans'], df_clustering['target_toxic'], \n                 normalize='index')\nprint(\"\\nProporción de toxicidad por cluster (KMeans):\")\ndisplay(ct)\n\n# Estadísticas por cluster\nprint(\"\\nEstadísticas por cluster:\")\ncluster_stats = df_clustering.groupby('cluster_kmeans').agg({\n    'target_toxic': ['count', 'mean', 'sum'],\n    'toxicity_score': ['mean', 'std'],\n    'authorFollowers': 'mean',\n    'content_length': 'mean'\n}).round(4)\n\ncluster_stats.columns = ['_'.join(col).strip() for col in cluster_stats.columns.values]\ndisplay(cluster_stats)\n\n\n\n--- ANÁLISIS CLUSTERS vs TARGET ---\n\nProporción de toxicidad por cluster (KMeans):\n\n\n\n\n\n\n\n\ntarget_toxic\n0\n1\n\n\ncluster_kmeans\n\n\n\n\n\n\n0\n0.888559\n0.111441\n\n\n1\n1.000000\n0.000000\n\n\n\n\n\n\n\n\nEstadísticas por cluster:\n\n\n\n\n\n\n\n\n\ntarget_toxic_count\ntarget_toxic_mean\ntarget_toxic_sum\ntoxicity_score_mean\ntoxicity_score_std\nauthorFollowers_mean\ncontent_length_mean\n\n\ncluster_kmeans\n\n\n\n\n\n\n\n\n\n\n\n0\n1346\n0.1114\n150\n0.2539\n0.244\n603.39\n123.8618\n\n\n1\n1\n0.0000\n0\n0.2137\nNaN\n4577730.00\n665.0000\n\n\n\n\n\n\n\nEl cluster 0 concentra la mayoría de los tweets (99%) y refleja una toxicidad media de 0.25, mientras que el cluster 1 tiene un único tweet no tóxico.\n\n\n\n\n\n\nCode\nprint(\"\\n--- Resumen rápido ---\")\nprint(\"Tamaño dataset tras limpiar toxicity_score:\", df.shape[0])\nprint(\"Umbral usado para binarizar toxicity:\", threshold)\nprint(\"Mejor k clustering por silhouette:\", best_k)\n\n\n\n--- Resumen rápido ---\nTamaño dataset tras limpiar toxicity_score: 1347\nUmbral usado para binarizar toxicity: 0.6\nMejor k clustering por silhouette: 2\n\n\n\nInicialmente se realizó una limpieza de los datos eliminando los valores nulos y columnas irrelevantes, esto permitió que la variable objetivo toxicity_score fue depurada, quedando 1,347 registros válidos para el análisis. Finalmente, el texto fue normalizado (minúsculas, sin URLs, menciones ni hashtags) y se aplicaron stopwords en español con NLTK para mejorar la calidad del texto.\nEl análisis estadístico reveló un fuerte desbalance en la distribución de la toxicidad: la mayoría de los tweets presentan valores bajos de toxicity_score, mientras que solo un pequeño porcentaje supera 0.6, por lo que se uso para clasificar los tweets entre tóxicos y no tóxicos, equilibrando la detección de casos sin perder representatividad.\nSe implementó Logistic Regression como modelo de clasificación binaria para identificar tweets tóxicos debido a la fortaleza que este tiene a la hora de hacer predicciones con valores binarios. El modelo alcanzó una precisión global del 85% y una ROC-AUC de 0.70, lo cual indica una capacidad moderada de distinguir entre ambas clases.\nSe implementaron dos modelos para predecir el valor continuo de toxicidad, en donde se identificó que Random Forest Regressor capturó relaciones no lineales y mostró un mejor rendimiento (menor MAE y mayor R²), indicando que la toxicidad depende de interacciones complejas entre características textuales y numéricas.",
    "crumbs": [
      "Proyecto Final"
    ]
  },
  {
    "objectID": "Pfinal.html#importar-librerias",
    "href": "Pfinal.html#importar-librerias",
    "title": "Proyecto Final",
    "section": "",
    "text": "Code\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score,\n                             f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay,\n                             mean_absolute_error, mean_squared_error, r2_score)\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import RocCurveDisplay\nfrom sklearn.pipeline import make_pipeline\n\nRANDOM_STATE = 42\n\nimport nltk\nfrom nltk.corpus import stopwords\nsns.set(style=\"whitegrid\")\nnltk.download('stopwords')\nstop_words = set(stopwords.words('spanish'))\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\StarMedia\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!",
    "crumbs": [
      "Proyecto Final"
    ]
  },
  {
    "objectID": "Pfinal.html#limpieza-de-los-datos",
    "href": "Pfinal.html#limpieza-de-los-datos",
    "title": "Proyecto Final",
    "section": "",
    "text": "Code\ndf = pd.read_csv('1500_tweets_con_toxicity.csv')\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1500 entries, 0 to 1499\nData columns (total 27 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   tweetId              1500 non-null   int64  \n 1   tweetUrl             1500 non-null   object \n 2   content              1500 non-null   object \n 3   isReply              1500 non-null   bool   \n 4   replyTo              1490 non-null   object \n 5   createdAt            1500 non-null   object \n 6   authorId             1500 non-null   int64  \n 7   authorName           1500 non-null   object \n 8   authorUsername       1500 non-null   object \n 9   authorVerified       1500 non-null   bool   \n 10  authorFollowers      1500 non-null   int64  \n 11  authorProfilePic     1500 non-null   object \n 12  authorJoinDate       1500 non-null   object \n 13  source               1500 non-null   object \n 14  hashtags             121 non-null    object \n 15  mentions             1499 non-null   object \n 16  conversationId       1500 non-null   int64  \n 17  inReplyToId          1500 non-null   int64  \n 18  Date                 1500 non-null   object \n 19  time_response        1500 non-null   float64\n 20  account_age_days     1500 non-null   int64  \n 21  mentions_count       1500 non-null   int64  \n 22  hashtags_count       1500 non-null   int64  \n 23  content_length       1500 non-null   int64  \n 24  has_profile_picture  1500 non-null   bool   \n 25  sentiment_polarity   1500 non-null   float64\n 26  toxicity_score       1347 non-null   float64\ndtypes: bool(3), float64(3), int64(9), object(12)\nmemory usage: 285.8+ KB\n\n\n\n\n\n\n\n\n\ntweetId\ntweetUrl\ncontent\nisReply\nreplyTo\ncreatedAt\nauthorId\nauthorName\nauthorUsername\nauthorVerified\n...\ninReplyToId\nDate\ntime_response\naccount_age_days\nmentions_count\nhashtags_count\ncontent_length\nhas_profile_picture\nsentiment_polarity\ntoxicity_score\n\n\n\n\n0\n1878630970745900800\nhttps://x.com/Pableins15/status/18786309707459...\n@DanielNoboaOk @DiegoBorjaPC Lávate el hocico ...\nTrue\nDanielNoboaOk\n2025-01-13 02:31:00\n176948611\nPablo Balarezo\nPableins15\nFalse\n...\n1878539079249547520\n2025-01-12 20:26:32\n364.466667\n5261\n2\n0\n309\nFalse\n0.0\n0.543256\n\n\n1\n1904041877503984128\nhttps://x.com/solma1201/status/190404187750398...\n@DanielNoboaOk De esa arrastrada no te levanta...\nTrue\nDanielNoboaOk\n2025-03-24 05:25:00\n1368663286582030336\nSolma1201\nsolma1201\nFalse\n...\n1904003201143115776\n2025-03-24 02:51:52\n153.133333\n1399\n1\n0\n70\nTrue\n0.0\n0.426917\n\n\n2\n1877463444649046016\nhttps://x.com/Mediterran67794/status/187746344...\n@LuisaGonzalezEc @RC5Oficial Protegiendo a los...\nTrue\nLuisaGonzalezEc\n2025-01-09 21:12:00\n1851005619106451712\nMédico Escritor Filósofo Hermeneútico\nMediterran67794\nFalse\n...\n1877158437236228352\n2025-01-09 01:00:22\n1211.633333\n68\n2\n0\n122\nTrue\n0.0\n0.555970\n\n\n3\n1881356046108885248\nhttps://x.com/ardededa/status/1881356046108885494\n@DanielNoboaOk #NoboaPresidente. Todo 7!\nTrue\nDanielNoboaOk\n2025-01-20 15:00:00\n315799544\nDenise\nardededa\nFalse\n...\n1881165128185560832\n2025-01-20 02:21:31\n758.483333\n4955\n1\n0\n41\nTrue\n0.0\n0.046615\n\n\n4\n1888331962063978752\nhttps://x.com/LMarquinezm/status/1888331962063...\n@slider1908 @LuisaGonzalezEc @DianaAtamaint @c...\nTrue\nslider1908\n2025-02-08 20:59:00\n1551883554\nLuis Marquínez\nLMarquinezm\nFalse\n...\n1888256000085397504\n2025-02-08 14:59:07\n359.883333\n4208\n5\n0\n101\nTrue\n0.0\n0.846027\n\n\n\n\n5 rows × 27 columns\n\n\n\n\n\nCode\nif 'tweetId' in df.columns:\n    df.drop_duplicates(subset='tweetId', inplace=True)\n\n# Revisar nulos por columna\nprint(\"Nulos por columna:\")\nprint(df.isnull().sum().sort_values(ascending=False).head(20))\n\n\nNulos por columna:\nhashtags            1379\ntoxicity_score       153\nreplyTo               10\nmentions               1\ncontent                0\ncreatedAt              0\nauthorId               0\nauthorName             0\nisReply                0\ntweetUrl               0\ntweetId                0\nauthorFollowers        0\nauthorVerified         0\nauthorUsername         0\nauthorJoinDate         0\nsource                 0\nconversationId         0\ninReplyToId            0\nauthorProfilePic       0\nDate                   0\ndtype: int64\n\n\nEn esta sección se realiza una inspección inicial de la estructura del conjunto de datos y se identifican los valores faltantes (nulos), puesto a que se sabe que el dataset tiene 1500 datos.\n\n\nPara mantener el control de los datos de la columna targe, que sera la columna toxicity_score, primero es necesario eliminar los valores nulos, puesto que la api, en ocaciones puede fallar, dejando filas sin este valor, dificultando su procesamiento dentro del modelo\n\n\nCode\ndf = df.dropna(subset=['toxicity_score'])\ndf.isnull().sum().sort_values(ascending=False)\n\n\nhashtags               1229\nreplyTo                   9\nmentions                  1\ncontent                   0\nisReply                   0\ncreatedAt                 0\nauthorId                  0\nauthorName                0\nauthorUsername            0\ntweetUrl                  0\ntweetId                   0\nauthorFollowers           0\nauthorVerified            0\nauthorJoinDate            0\nauthorProfilePic          0\nsource                    0\nconversationId            0\ninReplyToId               0\nDate                      0\ntime_response             0\naccount_age_days          0\nmentions_count            0\nhashtags_count            0\ncontent_length            0\nhas_profile_picture       0\nsentiment_polarity        0\ntoxicity_score            0\ndtype: int64\n\n\nAqui se puede observar como ya no existen valores nulos en la columna toxicity_score\n\n\n\n\n\nCode\ncolumns_to_keep = [\n    'content', 'isReply', 'authorVerified', 'has_profile_picture',\n    'authorFollowers', 'account_age_days', 'mentions_count',\n    'hashtags_count', 'content_length', 'sentiment_polarity',\n    'toxicity_score'\n]\ndf = df[columns_to_keep]\n\n\n\n\n\n\n\nCode\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub(r\"http\\S+\", \"\", text)         # URLs\n    text = re.sub(r\"@\\w+\", \"\", text)            # menciones\n    text = re.sub(r\"#\\w+\", \"\", text)            # hashtags\n    text = re.sub(r\"[^a-záéíóúñü\\s]\", \"\", text) # puntuación\n    text = \" \".join([word for word in text.split() if word not in stop_words])\n    return text\n\ndf['clean_content'] = df['content'].apply(clean_text)\n\n\nPara permitirme una mejor limpieza de los textos hice uso de stopwords, y para este caso en especifico de las stopwords que permite usar el nltk del idioma español",
    "crumbs": [
      "Proyecto Final"
    ]
  },
  {
    "objectID": "Pfinal.html#eda",
    "href": "Pfinal.html#eda",
    "title": "Proyecto Final",
    "section": "",
    "text": "Después de eliminar los valores nulos dentro de la columna objetivo, se puede observar que quedaron 1347 registros para su procesamiento\n\n\nCode\n# Estadísticas numéricas\nprint(df.describe().T)\n\nplt.figure(figsize=(8,4))\nsns.histplot(df['toxicity_score'], bins=30, kde=True)\nplt.title('Distribución de TOXICITY_SCORE')\nplt.xlabel('Toxicity score (0-1)')\nplt.show()\n\n\n                     count         mean            std       min         25%  \\\nauthorFollowers     1347.0  4001.405345  124989.608109   0.00000    6.500000   \naccount_age_days    1347.0  2233.675575    1980.549465 -90.00000  447.000000   \nmentions_count      1347.0     1.747587       0.960660   0.00000    1.000000   \nhashtags_count      1347.0     0.000000       0.000000   0.00000    0.000000   \ncontent_length      1347.0   124.263549      77.691218  20.00000   66.000000   \nsentiment_polarity  1347.0    -0.008434       0.122688  -1.00000    0.000000   \ntoxicity_score      1347.0     0.253879       0.243942   0.00194    0.028444   \n\n                            50%          75%           max  \nauthorFollowers       39.000000   186.000000  4.577730e+06  \naccount_age_days    1438.000000  4372.000000  6.506000e+03  \nmentions_count         2.000000     2.000000  1.000000e+01  \nhashtags_count         0.000000     0.000000  0.000000e+00  \ncontent_length       102.000000   158.500000  6.840000e+02  \nsentiment_polarity     0.000000     0.000000  1.000000e+00  \ntoxicity_score         0.188392     0.426917  9.391453e-01  \n\n\n\n\n\n\n\n\n\nEste grafico nos da a entender como es la distribucion de la cantidad de toxicidad en los registros, mostrando que por lo general se evita tener una gran toxicidad\n\n\n\n\n\nCode\nplt.figure(figsize=(6,3))\nsns.boxplot(x=df['toxicity_score'])\nplt.title('Boxplot TOXICITY_SCORE')\nplt.show()\n\n\n\n\n\n\n\n\n\nEl boxplot se observa una dispersión limitada y muy pocos outliers dejando a la vista que escoger un valor mayor a 0.5 será lo mejor para poder seleccionar si un comentario es toxico o no\n\n\n\n\n\nCode\nif 'isReply' in df.columns:\n    plt.figure(figsize=(6,3))\n    sns.countplot(x='isReply', data=df)\n    plt.title('isReply distribución')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nTras observar la distribución de toxicity_score (histograma y boxplot), se decide usar umbral = 0.6 como punto incial para considerar ‘tóxico’ porque suele marcar la zona donde la densidad decrece significativamente hacia valores altos.\n\n\nCode\nthreshold = 0.6\ndf['target_toxic'] = (df['toxicity_score'] &gt;= threshold).astype(int)\n\n# Ver la proporción de clases\nprint(\"Distribución binaria (0=no tóxico, 1=tóxico):\")\nprint(df['target_toxic'].value_counts(normalize=True))\nsns.countplot(x='target_toxic', data=df)\nplt.title(f'Distribución target con umbral = {threshold}')\nplt.show()\n\n\nDistribución binaria (0=no tóxico, 1=tóxico):\ntarget_toxic\n0    0.888641\n1    0.111359\nName: proportion, dtype: float64\n\n\n\n\n\n\n\n\n\nEl gráfico de barras evidencia un fuerte desbalance: aproximadamente 89% no tóxicos y 11% tóxicos. Con esto se avanzo para realizar el procesamiento de los datos.",
    "crumbs": [
      "Proyecto Final"
    ]
  },
  {
    "objectID": "Pfinal.html#preprocesamiento-y-codificación",
    "href": "Pfinal.html#preprocesamiento-y-codificación",
    "title": "Proyecto Final",
    "section": "",
    "text": "Code\nfeatures = [\n    'clean_content',          # texto procesado\n    'authorVerified',\n    'has_profile_picture',\n    'isReply',\n    'authorFollowers',\n    'account_age_days',\n    'mentions_count',\n    'hashtags_count', \n    'content_length', \n    'sentiment_polarity'\n]\n\ntext_feature = ['clean_content']\ncategorical_features = ['isReply', 'authorVerified', 'has_profile_picture']\nnumeric_features = [\n    'authorFollowers', 'account_age_days', 'mentions_count',\n    'hashtags_count', 'content_length', 'sentiment_polarity'\n]\nprint(['clean_content'])\n\n\n['clean_content']\n\n\n\n\n\n\n\nCode\nspanish_stopwords = stopwords.words('spanish')\ntfidf = TfidfVectorizer(stop_words=spanish_stopwords)\ncat_encoder = OneHotEncoder(handle_unknown='ignore')\nscaler = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', tfidf, 'clean_content'),\n        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n        ('num', StandardScaler(), numeric_features)\n    ],\n    remainder='drop'\n)\n\n\nSe aplicaron transformaciones con TF-IDF para texto, escalado para variables numéricas y codificación one-hot para categóricas, integradas en un ColumnTransformer.\n\n\n\n\n\nCode\nX = df[['clean_content'] + categorical_features + numeric_features]\ny_class = df['target_toxic']        # para clasificación binaria\ny_reg = df['toxicity_score']        # para regresión (continuo)\n\nX_train, X_test, y_train_cl, y_test_cl = train_test_split(\n    X, y_class, test_size=0.2, random_state=RANDOM_STATE, stratify=y_class\n)\n\n# Para regresión (mismo split de X, pero con y_reg alineado):\n# usamos el mismo índice de train/test para evitar fugas:\n_, _, y_train_reg, y_test_reg = train_test_split(\n    X, y_reg, test_size=0.2, random_state=RANDOM_STATE, stratify=y_class\n)",
    "crumbs": [
      "Proyecto Final"
    ]
  },
  {
    "objectID": "Pfinal.html#clasificación",
    "href": "Pfinal.html#clasificación",
    "title": "Proyecto Final",
    "section": "",
    "text": "Para la clasificación se entrenó un modelo de regresión logística. Esto debido a que el tipo de datos que se puede buscar es un binario. 1 es toxico, 2 no lo es.\n\n\n\n\nCode\nclf_logistic = Pipeline([\n    ('preproc', preprocessor),\n    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', random_state=RANDOM_STATE))\n])\n\n# Entrenar\nclf_logistic.fit(X_train, y_train_cl)\n\n\nPipeline(steps=[('preproc',\n                 ColumnTransformer(transformers=[('text',\n                                                  TfidfVectorizer(stop_words=['de',\n                                                                              'la',\n                                                                              'que',\n                                                                              'el',\n                                                                              'en',\n                                                                              'y',\n                                                                              'a',\n                                                                              'los',\n                                                                              'del',\n                                                                              'se',\n                                                                              'las',\n                                                                              'por',\n                                                                              'un',\n                                                                              'para',\n                                                                              'con',\n                                                                              'no',\n                                                                              'una',\n                                                                              'su',\n                                                                              'al',\n                                                                              'lo',\n                                                                              'como',\n                                                                              'más',\n                                                                              'pero',\n                                                                              'sus',\n                                                                              'le',\n                                                                              'ya',\n                                                                              'o',\n                                                                              'este',\n                                                                              'sí',\n                                                                              'porque', ...]),\n                                                  'clean_content'),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['isReply', 'authorVerified',\n                                                   'has_profile_picture']),\n                                                 ('num', StandardScaler(),\n                                                  ['authorFollowers',\n                                                   'account_age_days',\n                                                   'mentions_count',\n                                                   'hashtags_count',\n                                                   'content_length',\n                                                   'sentiment_polarity'])])),\n                ('clf',\n                 LogisticRegression(class_weight='balanced', max_iter=2000,\n                                    random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('preproc', ...), ('clf', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    preproc: ColumnTransformer?Documentation for preproc: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('text', ...), ('cat', ...), ...]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    textclean_contentTfidfVectorizer?Documentation for TfidfVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ninput \n'content'\n\n\n\nencoding \n'utf-8'\n\n\n\ndecode_error \n'strict'\n\n\n\nstrip_accents \nNone\n\n\n\nlowercase \nTrue\n\n\n\npreprocessor \nNone\n\n\n\ntokenizer \nNone\n\n\n\nanalyzer \n'word'\n\n\n\nstop_words \n['de', 'la', ...]\n\n\n\ntoken_pattern \n'(?u)\\\\b\\\\w\\\\w+\\\\b'\n\n\n\nngram_range \n(1, ...)\n\n\n\nmax_df \n1.0\n\n\n\nmin_df \n1\n\n\n\nmax_features \nNone\n\n\n\nvocabulary \nNone\n\n\n\nbinary \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nnorm \n'l2'\n\n\n\nuse_idf \nTrue\n\n\n\nsmooth_idf \nTrue\n\n\n\nsublinear_tf \nFalse\n\n\n\n\n            \n        \n    cat['isReply', 'authorVerified', 'has_profile_picture']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    num['authorFollowers', 'account_age_days', 'mentions_count', 'hashtags_count', 'content_length', 'sentiment_polarity']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \n'balanced'\n\n\n\nrandom_state \n42\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n2000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_pred_log = clf_logistic.predict(X_test)\ny_proba_log = clf_logistic.predict_proba(X_test)[:,1]\n\nprint(\"Logistic Regression:\")\nprint(\"Accuracy:\", accuracy_score(y_test_cl, y_pred_log))\nprint(\"Precision:\", precision_score(y_test_cl, y_pred_log, zero_division=0))\nprint(\"Recall:\", recall_score(y_test_cl, y_pred_log, zero_division=0))\nprint(\"F1:\", f1_score(y_test_cl, y_pred_log, zero_division=0))\nprint(\"ROC-AUC:\", roc_auc_score(y_test_cl, y_proba_log))\n\n\nLogistic Regression:\nAccuracy: 0.8518518518518519\nPrecision: 0.3333333333333333\nRecall: 0.3333333333333333\nF1: 0.3333333333333333\nROC-AUC: 0.7061111111111111\n\n\nSe obtuvo una precisión aceptable (~85%) pero con métricas de recall y F1 moderadas debido al desbalance de clases, ya que existe en su gran mayoría un mayor número de comentarios no tóxicos, frente a los tóxicos.\n\n\n\n\n\nCode\ncm = confusion_matrix(y_test_cl, y_pred_log)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no_tox','tox'])\ndisp.plot(cmap='Blues')\nplt.title('Confusion Matrix - Logistic')\nplt.show()\n\n\n\n\n\n\n\n\n\nMuestra una buena detección de la clase no tóxica, aunque algunos falsos negativos persisten.\n\n\n\n\n\nCode\nRocCurveDisplay.from_estimator(clf_logistic, X_test, y_test_cl)\nplt.title('Logistic')\nplt.show()\n\n\n\n\n\n\n\n\n\nEl área bajo la curva (ROC-AUC ≈ 0.70) refleja un rendimiento medio, con capacidad moderada de distinguir entre tweets tóxicos y no tóxicos.",
    "crumbs": [
      "Proyecto Final"
    ]
  },
  {
    "objectID": "Pfinal.html#regresión",
    "href": "Pfinal.html#regresión",
    "title": "Proyecto Final",
    "section": "",
    "text": "Se implementaron modelos LinearRegression y RandomForestRegressor para predecir la toxicidad continua. El Random Forest mostró mejor ajuste (menor MAE y mayor R²). Se utilizaron 2 modelos para poder evaluar si la toxicidad puede explicarse de forma lineal o si requiere un enfoque más flexible. En este caso, el Random Forest mostró mejor rendimiento (menor error y mayor R²), lo que sugiere relaciones más complejas en los datos.\n\n\n\n\nCode\nreg_lin = Pipeline([\n    ('preproc', preprocessor),\n    ('linreg', LinearRegression())\n])\n\nreg_rf = Pipeline([\n    ('preproc', preprocessor),\n    ('rf', RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE))\n])\n\n# Entrenar\nreg_lin.fit(X_train, y_train_reg)\nreg_rf.fit(X_train, y_train_reg)\n\n\nPipeline(steps=[('preproc',\n                 ColumnTransformer(transformers=[('text',\n                                                  TfidfVectorizer(stop_words=['de',\n                                                                              'la',\n                                                                              'que',\n                                                                              'el',\n                                                                              'en',\n                                                                              'y',\n                                                                              'a',\n                                                                              'los',\n                                                                              'del',\n                                                                              'se',\n                                                                              'las',\n                                                                              'por',\n                                                                              'un',\n                                                                              'para',\n                                                                              'con',\n                                                                              'no',\n                                                                              'una',\n                                                                              'su',\n                                                                              'al',\n                                                                              'lo',\n                                                                              'como',\n                                                                              'más',\n                                                                              'pero',\n                                                                              'sus',\n                                                                              'le',\n                                                                              'ya',\n                                                                              'o',\n                                                                              'este',\n                                                                              'sí',\n                                                                              'porque', ...]),\n                                                  'clean_content'),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  ['isReply', 'authorVerified',\n                                                   'has_profile_picture']),\n                                                 ('num', StandardScaler(),\n                                                  ['authorFollowers',\n                                                   'account_age_days',\n                                                   'mentions_count',\n                                                   'hashtags_count',\n                                                   'content_length',\n                                                   'sentiment_polarity'])])),\n                ('rf',\n                 RandomForestRegressor(n_estimators=200, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('preproc', ...), ('rf', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    preproc: ColumnTransformer?Documentation for preproc: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('text', ...), ('cat', ...), ...]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    textclean_contentTfidfVectorizer?Documentation for TfidfVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ninput \n'content'\n\n\n\nencoding \n'utf-8'\n\n\n\ndecode_error \n'strict'\n\n\n\nstrip_accents \nNone\n\n\n\nlowercase \nTrue\n\n\n\npreprocessor \nNone\n\n\n\ntokenizer \nNone\n\n\n\nanalyzer \n'word'\n\n\n\nstop_words \n['de', 'la', ...]\n\n\n\ntoken_pattern \n'(?u)\\\\b\\\\w\\\\w+\\\\b'\n\n\n\nngram_range \n(1, ...)\n\n\n\nmax_df \n1.0\n\n\n\nmin_df \n1\n\n\n\nmax_features \nNone\n\n\n\nvocabulary \nNone\n\n\n\nbinary \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nnorm \n'l2'\n\n\n\nuse_idf \nTrue\n\n\n\nsmooth_idf \nTrue\n\n\n\nsublinear_tf \nFalse\n\n\n\n\n            \n        \n    cat['isReply', 'authorVerified', 'has_profile_picture']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nFalse\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    num['authorFollowers', 'account_age_days', 'mentions_count', 'hashtags_count', 'content_length', 'sentiment_polarity']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    RandomForestRegressor?Documentation for RandomForestRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n200\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n1.0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n42\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_pred_lin = reg_lin.predict(X_test)\ny_pred_rf = reg_rf.predict(X_test)\n\n# Métricas\ndef regression_metrics(y_true, y_pred, label=\"Model\"):\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    print(f\"{label} -&gt; MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n\nregression_metrics(y_test_reg, y_pred_lin, \"LinearRegression\")\n\n\nLinearRegression -&gt; MAE: 0.1478, RMSE: 0.0407, R2: 0.3225\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(6,6))\nplt.scatter(y_test_reg, y_pred_rf, alpha=0.5)\nplt.plot([0,1],[0,1], 'r--')  # línea ideal\nplt.xlabel(\"Toxicity real\")\nplt.ylabel(\"Toxicity predicho\")\nplt.title(\"Real vs Predicho - RandomForestRegressor\")\nplt.show()\n\n\n\n\n\n\n\n\n\nEl scatter muestra que la mayoría de predicciones del Random Forest siguen la línea ideal, indicando un buen ajuste general.\n\n\n\n\n\nCode\nresid = y_test_reg - y_pred_rf\nplt.figure(figsize=(6,4))\nsns.histplot(resid, bins=30, kde=True)\nplt.title(\"Errores residuales - RandomForestRegressor\")\nplt.xlabel(\"Residual (real - predicho)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLos residuos se concentran cerca de cero, lo que sugiere ausencia de sesgos importantes en las predicciones del modelo.",
    "crumbs": [
      "Proyecto Final"
    ]
  },
  {
    "objectID": "Pfinal.html#clustering",
    "href": "Pfinal.html#clustering",
    "title": "Proyecto Final",
    "section": "",
    "text": "Code\n# 1. Procesar texto con TF-IDF\ntfidf_clust = TfidfVectorizer(max_features=500, stop_words=spanish_stopwords, ngram_range=(1,2))\nX_text_tfidf = tfidf_clust.fit_transform(df['clean_content'])\n\nprint(f\"TF-IDF shape: {X_text_tfidf.shape}\")\n\n# 2. Reducir dimensionalidad del texto con SVD (manejar caso de pocas features)\nn_components = min(20, X_text_tfidf.shape[1] - 1)\nif n_components &lt; 2:\n    n_components = 1\n    print(\" Pocas características en TF-IDF, usando 1 componente\")\nelse:\n    print(f\"Usando {n_components} componentes para SVD\")\n\nsvd = TruncatedSVD(n_components=n_components, random_state=RANDOM_STATE)\nX_text_reduced = svd.fit_transform(X_text_tfidf)\nprint(f\"Texto reducido: {X_text_reduced.shape}\")\n\n# 3. Procesar variables categóricas\ncat_encoder_clust = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nX_cat_encoded = cat_encoder_clust.fit_transform(df[categorical_features])\nprint(f\"Categóricas: {X_cat_encoded.shape}\")\n\n# 4. Estandarizar variables numéricas\nscaler_clust = StandardScaler()\nX_num_scaled = scaler_clust.fit_transform(df[numeric_features])\nprint(f\"Numéricas: {X_num_scaled.shape}\")\n\n# 5. Combinar todas las características\nX_clust = np.hstack([X_text_reduced, X_cat_encoded, X_num_scaled])\nprint(f\"Forma final para clustering: {X_clust.shape}\")\n\n\nTF-IDF shape: (1347, 500)\nUsando 20 componentes para SVD\nTexto reducido: (1347, 20)\nCategóricas: (1347, 4)\nNuméricas: (1347, 6)\nForma final para clustering: (1347, 30)\n\n\nSe usó TF-IDF + SVD para reducir texto y se combinaron con variables categóricas y numéricas.\n\n\n\n\n\nCode\nsil_scores = {}\nfor k in range(2, 7):\n    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n    labels = kmeans.fit_predict(X_clust)\n    sil = silhouette_score(X_clust, labels)\n    sil_scores[k] = sil\n    print(f\"k={k}, silhouette={sil:.4f}\")\n\nbest_k = max(sil_scores, key=sil_scores.get)\nprint(f\"Mejor k por silhouette: {best_k}\")\n\n\nk=2, silhouette=0.9320\nk=3, silhouette=0.2795\nk=4, silhouette=0.3047\nk=5, silhouette=0.3333\nk=6, silhouette=0.3479\nMejor k por silhouette: 2\n\n\nSe probaron valores de k entre 2 y 6; el mejor fue k = 2 (silhouette = 0.93).\n\n\n\n\n\nCode\nkmeans_final = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=10)\ncluster_labels = kmeans_final.fit_predict(X_clust)\n\n# Añadir etiquetas de cluster al dataframe\ndf_clustering = df.copy()\ndf_clustering['cluster_kmeans'] = cluster_labels\n\n\n\n\n\n\n\nCode\n# Reducir a 2D para visualización\npca = PCA(n_components=2, random_state=RANDOM_STATE)\nX_2d = pca.fit_transform(X_clust)\n\nplt.figure(figsize=(15, 5))\n\n# Subplot 1: Clusters de KMeans\nplt.subplot(1, 3, 1)\nscatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, cmap='tab10', alpha=0.6, s=30)\nplt.colorbar(scatter)\nplt.title(f'KMeans Clusters (k={best_k})')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\n\n# Subplot 2: Target binario (toxicidad)\nplt.subplot(1, 3, 2)\ncolors = ['blue', 'red']  # 0=no tóxico, 1=tóxico\nfor target_val in [0, 1]:\n    mask = df_clustering['target_toxic'] == target_val\n    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], \n                c=colors[target_val], label=f'Toxicity={target_val}', alpha=0.6, s=30)\nplt.legend()\nplt.title('Distribución por Toxicidad')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\n\n# Subplot 3: Comparación clusters vs toxicidad\nplt.subplot(1, 3, 3)\nfor cluster_id in range(best_k):\n    mask = df_clustering['cluster_kmeans'] == cluster_id\n    toxic_ratio = df_clustering[mask]['target_toxic'].mean()\n    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], \n                c=['red' if toxic_ratio &gt; 0.5 else 'blue'], \n                label=f'Cluster {cluster_id}', alpha=0.6, s=30)\nplt.title('Clusters coloreados por toxicidad predominante')\nplt.xlabel('Componente Principal 1')\nplt.ylabel('Componente Principal 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nLos gráficos PCA en 2D muestran:\n\nDistribución clara entre los dos clusters (izquierda).\nSeparación moderada entre tweets tóxicos y no tóxicos (centro).\nDiferencias visibles en toxicidad promedio por cluster (derecha).\n\n\n\n\n\n\nCode\nprint(\"\\n--- ANÁLISIS CLUSTERS vs TARGET ---\")\n\n# Tabla de contingencia\nct = pd.crosstab(df_clustering['cluster_kmeans'], df_clustering['target_toxic'], \n                 normalize='index')\nprint(\"\\nProporción de toxicidad por cluster (KMeans):\")\ndisplay(ct)\n\n# Estadísticas por cluster\nprint(\"\\nEstadísticas por cluster:\")\ncluster_stats = df_clustering.groupby('cluster_kmeans').agg({\n    'target_toxic': ['count', 'mean', 'sum'],\n    'toxicity_score': ['mean', 'std'],\n    'authorFollowers': 'mean',\n    'content_length': 'mean'\n}).round(4)\n\ncluster_stats.columns = ['_'.join(col).strip() for col in cluster_stats.columns.values]\ndisplay(cluster_stats)\n\n\n\n--- ANÁLISIS CLUSTERS vs TARGET ---\n\nProporción de toxicidad por cluster (KMeans):\n\n\n\n\n\n\n\n\ntarget_toxic\n0\n1\n\n\ncluster_kmeans\n\n\n\n\n\n\n0\n0.888559\n0.111441\n\n\n1\n1.000000\n0.000000\n\n\n\n\n\n\n\n\nEstadísticas por cluster:\n\n\n\n\n\n\n\n\n\ntarget_toxic_count\ntarget_toxic_mean\ntarget_toxic_sum\ntoxicity_score_mean\ntoxicity_score_std\nauthorFollowers_mean\ncontent_length_mean\n\n\ncluster_kmeans\n\n\n\n\n\n\n\n\n\n\n\n0\n1346\n0.1114\n150\n0.2539\n0.244\n603.39\n123.8618\n\n\n1\n1\n0.0000\n0\n0.2137\nNaN\n4577730.00\n665.0000\n\n\n\n\n\n\n\nEl cluster 0 concentra la mayoría de los tweets (99%) y refleja una toxicidad media de 0.25, mientras que el cluster 1 tiene un único tweet no tóxico.",
    "crumbs": [
      "Proyecto Final"
    ]
  },
  {
    "objectID": "Pfinal.html#conclusiones",
    "href": "Pfinal.html#conclusiones",
    "title": "Proyecto Final",
    "section": "",
    "text": "Code\nprint(\"\\n--- Resumen rápido ---\")\nprint(\"Tamaño dataset tras limpiar toxicity_score:\", df.shape[0])\nprint(\"Umbral usado para binarizar toxicity:\", threshold)\nprint(\"Mejor k clustering por silhouette:\", best_k)\n\n\n\n--- Resumen rápido ---\nTamaño dataset tras limpiar toxicity_score: 1347\nUmbral usado para binarizar toxicity: 0.6\nMejor k clustering por silhouette: 2\n\n\n\nInicialmente se realizó una limpieza de los datos eliminando los valores nulos y columnas irrelevantes, esto permitió que la variable objetivo toxicity_score fue depurada, quedando 1,347 registros válidos para el análisis. Finalmente, el texto fue normalizado (minúsculas, sin URLs, menciones ni hashtags) y se aplicaron stopwords en español con NLTK para mejorar la calidad del texto.\nEl análisis estadístico reveló un fuerte desbalance en la distribución de la toxicidad: la mayoría de los tweets presentan valores bajos de toxicity_score, mientras que solo un pequeño porcentaje supera 0.6, por lo que se uso para clasificar los tweets entre tóxicos y no tóxicos, equilibrando la detección de casos sin perder representatividad.\nSe implementó Logistic Regression como modelo de clasificación binaria para identificar tweets tóxicos debido a la fortaleza que este tiene a la hora de hacer predicciones con valores binarios. El modelo alcanzó una precisión global del 85% y una ROC-AUC de 0.70, lo cual indica una capacidad moderada de distinguir entre ambas clases.\nSe implementaron dos modelos para predecir el valor continuo de toxicidad, en donde se identificó que Random Forest Regressor capturó relaciones no lineales y mostró un mejor rendimiento (menor MAE y mayor R²), indicando que la toxicidad depende de interacciones complejas entre características textuales y numéricas.",
    "crumbs": [
      "Proyecto Final"
    ]
  }
]